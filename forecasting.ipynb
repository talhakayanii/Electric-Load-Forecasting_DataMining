{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electricity Demand Forecasting\n",
    "# Next-day Hourly Electricity Demand Prediction using Weather and Temporal Features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# For modeling\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# For neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# For ensemble stacking\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set plot styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# 1. Data Loading and Initial Exploration\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('merged_weather_demand_final.csv')\n",
    "\n",
    "# Convert DateTime to datetime format\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "print(f\"Dataset period: {df['DateTime'].min()} to {df['DateTime'].max()}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values if any\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # For categorical columns, fill with mode\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    print(\"After handling missing values:\")\n",
    "    print(df.isnull().sum().sum())\n",
    "\n",
    "\n",
    "\n",
    "# 3. Feature Engineering\n",
    "\n",
    "print(\"\\nPerforming feature engineering...\")\n",
    "\n",
    "# 3.1 Create lag features (previous days demand)\n",
    "df_features = df.copy().sort_values('DateTime')\n",
    "\n",
    "# Add lag features (previous 24 hours demand) \n",
    "df_features['demand_lag_24h'] = df_features['Demand (MW)'].shift(24)\n",
    "\n",
    "# Add lag features for previous day same hour (24, 48, 72 hours)\n",
    "for i in range(1, 4):\n",
    "    df_features[f'demand_lag_{i}d_same_hour'] = df_features['Demand (MW)'].shift(i*24)\n",
    "\n",
    "# Add moving averages for smoothed signals\n",
    "df_features['demand_ma_24h'] = df_features['Demand (MW)'].rolling(window=24).mean()\n",
    "df_features['demand_ma_7d'] = df_features['Demand (MW)'].rolling(window=24*7).mean()\n",
    "\n",
    "# Add daily min, max, and range\n",
    "df_features['day_of_year'] = df_features['DateTime'].dt.dayofyear\n",
    "daily_stats = df_features.groupby('day_of_year')['Demand (MW)'].agg(['min', 'max'])\n",
    "daily_stats['range'] = daily_stats['max'] - daily_stats['min']\n",
    "df_features = df_features.merge(daily_stats, on='day_of_year', how='left')\n",
    "df_features.rename(columns={'min': 'daily_min', 'max': 'daily_max', 'range': 'daily_range'}, inplace=True)\n",
    "\n",
    "# Add Cyclical Features for hour, day of week, month, season\n",
    "df_features['hour_sin'] = np.sin(2 * np.pi * df_features['Hour']/24)\n",
    "df_features['hour_cos'] = np.cos(2 * np.pi * df_features['Hour']/24)\n",
    "df_features['weekday_sin'] = np.sin(2 * np.pi * df_features['Weekday']/7)\n",
    "df_features['weekday_cos'] = np.cos(2 * np.pi * df_features['Weekday']/7)\n",
    "df_features['month_sin'] = np.sin(2 * np.pi * df_features['Month']/12)\n",
    "df_features['month_cos'] = np.cos(2 * np.pi * df_features['Month']/12)\n",
    "\n",
    "# Weather derivatives\n",
    "df_features['temp_change_24h'] = df_features['Temperature (F)'] - df_features['Temperature (F)'].shift(24)\n",
    "\n",
    "# Create peak/off-peak indicator (6am-10pm is peak)\n",
    "df_features['is_peak_hours'] = ((df_features['Hour'] >= 6) & (df_features['Hour'] <= 22)).astype(int)\n",
    "\n",
    "# Drop rows with NaN due to lag features\n",
    "df_features.dropna(inplace=True)\n",
    "print(f\"After feature engineering: {df_features.shape}\")\n",
    "\n",
    "# 4. Data Preparation for Modeling\n",
    "\n",
    "# 4.1 Define target variable and features\n",
    "target = 'Demand (MW)'\n",
    "\n",
    "# 4.2 Select relevant features for modeling\n",
    "categorical_features = ['Weekday', 'Month', 'Season', 'Sub-Region']\n",
    "numerical_features = [\n",
    "    'Temperature (F)', 'Humidity', 'Wind Speed (mph)', 'Pressure', 'UV Index',\n",
    "    'Cloud Cover', 'Hour', 'hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos',\n",
    "    'month_sin', 'month_cos', 'is_peak_hours', 'demand_lag_24h',\n",
    "    'demand_lag_1d_same_hour', 'demand_lag_2d_same_hour', 'demand_lag_3d_same_hour',\n",
    "    'temp_change_24h', 'demand_ma_24h', 'demand_ma_7d'\n",
    "]\n",
    "\n",
    "all_features = numerical_features + categorical_features\n",
    "\n",
    "# 4.3 Train-test split (by date to avoid data leakage)\n",
    "split_date = df_features['DateTime'].max() - timedelta(days=60)  # Last 60 days as test set\n",
    "train_df = df_features[df_features['DateTime'] <= split_date]\n",
    "test_df = df_features[df_features['DateTime'] > split_date]\n",
    "\n",
    "print(f\"Training data: {train_df.shape} (From {train_df['DateTime'].min()} to {train_df['DateTime'].max()})\")\n",
    "print(f\"Testing data: {test_df.shape} (From {test_df['DateTime'].min()} to {test_df['DateTime'].max()})\")\n",
    "\n",
    "# 4.4 Define preprocessing pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# 4.5 Prepare X and y\n",
    "X_train = train_df[all_features]\n",
    "y_train = train_df[target]\n",
    "X_test = test_df[all_features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "# Store DateTime for plotting\n",
    "test_dates = test_df['DateTime']\n",
    "\n",
    "# 5. Model Implementation and Evaluation\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_test, y_test, model_name, naive_mae=None):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"MAE: {mae:.2f} MW\")\n",
    "    print(f\"RMSE: {rmse:.2f} MW\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    if naive_mae:\n",
    "        improvement = ((naive_mae - mae) / naive_mae) * 100\n",
    "        print(f\"Improvement over naive baseline: {improvement:.2f}%\")\n",
    "\n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_dates, y_test, label='Actual Demand', alpha=0.7)\n",
    "    plt.plot(test_dates, y_pred, label='Predicted Demand', alpha=0.7)\n",
    "    plt.title(f'{model_name}: Actual vs Predicted Demand')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand (MW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.lower().replace(\" \", \"_\")}_prediction.png')\n",
    "\n",
    "    # Return performance metrics and predictions\n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'mape': mape,\n",
    "        'r2': r2,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "print(\"\\nImplementing forecasting models...\")\n",
    "\n",
    "# 5.1 Naive Baseline (Previous day's same hour)\n",
    "print(\"\\nCreating naive baseline model...\")\n",
    "\n",
    "# Create a test set with the naive prediction (previous day same hour)\n",
    "naive_pred = test_df['demand_lag_1d_same_hour'].values\n",
    "naive_mae = mean_absolute_error(y_test, naive_pred)\n",
    "naive_rmse = np.sqrt(mean_squared_error(y_test, naive_pred))\n",
    "naive_mape = np.mean(np.abs((y_test - naive_pred) / y_test)) * 100\n",
    "\n",
    "print(f\"\\nNaive Baseline Performance:\")\n",
    "print(f\"MAE: {naive_mae:.2f} MW\")\n",
    "print(f\"RMSE: {naive_rmse:.2f} MW\")\n",
    "print(f\"MAPE: {naive_mape:.2f}%\")\n",
    "\n",
    "# Plot baseline\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_dates, y_test, label='Actual Demand', alpha=0.7)\n",
    "plt.plot(test_dates, naive_pred, label='Naive Forecast', alpha=0.7)\n",
    "plt.title('Naive Baseline: Actual vs Predicted Demand')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Demand (MW)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('naive_baseline_prediction.png')\n",
    "\n",
    "# Store results for comparison\n",
    "results = []\n",
    "predictions = {}\n",
    "predictions['Naive Baseline'] = naive_pred\n",
    "\n",
    "# 5.2 Linear Regression\n",
    "print(\"\\nTraining Linear Regression model...\")\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', Ridge(alpha=0.1))  # Ridge regression for better stability\n",
    "])\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "lr_results = evaluate_model(lr_pipeline, X_test, y_test, \"Linear Regression\", naive_mae)\n",
    "results.append(lr_results)\n",
    "predictions['Linear Regression'] = lr_results['y_pred']\n",
    "\n",
    "# 5.3 Random Forest Regressor\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1))\n",
    "])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "rf_results = evaluate_model(rf_pipeline, X_test, y_test, \"Random Forest\", naive_mae)\n",
    "results.append(rf_results)\n",
    "predictions['Random Forest'] = rf_results['y_pred']\n",
    "\n",
    "# 5.4 Gradient Boosting\n",
    "print(\"\\nTraining Gradient Boosting model...\")\n",
    "gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42))\n",
    "])\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "gb_results = evaluate_model(gb_pipeline, X_test, y_test, \"Gradient Boosting\", naive_mae)\n",
    "results.append(gb_results)\n",
    "predictions['Gradient Boosting'] = gb_results['y_pred']\n",
    "\n",
    "# 5.5 XGBoost\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                              random_state=42, n_jobs=-1))\n",
    "])\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "xgb_results = evaluate_model(xgb_pipeline, X_test, y_test, \"XGBoost\", naive_mae)\n",
    "results.append(xgb_results)\n",
    "predictions['XGBoost'] = xgb_results['y_pred']\n",
    "\n",
    "# 5.6 Simple Neural Network (Feedforward)\n",
    "\n",
    "try:\n",
    "    print(\"\\nTraining Feedforward Neural Network...\")\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "    X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "    # Build model\n",
    "    nn_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train_preprocessed.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile and train\n",
    "    nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    nn_model.fit(\n",
    "        X_train_preprocessed, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Make predictions\n",
    "    nn_pred = nn_model.predict(X_test_preprocessed).flatten()\n",
    "\n",
    "    # Calculate metrics\n",
    "    nn_mae = mean_absolute_error(y_test, nn_pred)\n",
    "    nn_rmse = np.sqrt(mean_squared_error(y_test, nn_pred))\n",
    "    nn_mape = np.mean(np.abs((y_test - nn_pred) / y_test)) * 100\n",
    "    nn_r2 = r2_score(y_test, nn_pred)\n",
    "\n",
    "    print(f\"\\nNeural Network Performance:\")\n",
    "    print(f\"MAE: {nn_mae:.2f} MW\")\n",
    "    print(f\"RMSE: {nn_rmse:.2f} MW\")\n",
    "    print(f\"MAPE: {nn_mape:.2f}%\")\n",
    "    print(f\"R²: {nn_r2:.4f}\")\n",
    "\n",
    "    # Calculate improvement over baseline\n",
    "    nn_improvement = ((naive_mae - nn_mae) / naive_mae) * 100\n",
    "    print(f\"Improvement over naive baseline: {nn_improvement:.2f}%\")\n",
    "\n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_dates, y_test, label='Actual Demand', alpha=0.7)\n",
    "    plt.plot(test_dates, nn_pred, label='Predicted Demand', alpha=0.7)\n",
    "    plt.title('Neural Network: Actual vs Predicted Demand')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand (MW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('neural_network_prediction.png')\n",
    "\n",
    "    # Store results\n",
    "    nn_results = {\n",
    "        'model_name': 'Neural Network',\n",
    "        'mae': nn_mae,\n",
    "        'rmse': nn_rmse,\n",
    "        'mape': nn_mape,\n",
    "        'r2': nn_r2,\n",
    "        'y_pred': nn_pred\n",
    "    }\n",
    "    results.append(nn_results)\n",
    "    predictions['Neural Network'] = nn_pred\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error training neural network: {e}\")\n",
    "    pass\n",
    "\n",
    "# 6. Ensemble Models (Requirement)\n",
    "\n",
    "print(\"\\nImplementing ensemble models...\")\n",
    "\n",
    "# 6.1 Stacking Ensemble\n",
    "print(\"\\nTraining Stacking Ensemble model...\")\n",
    "\n",
    "# Define base models for stacking\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = Ridge(alpha=0.1)\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessor\n",
    "stacking_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', stacking_model)\n",
    "])\n",
    "\n",
    "# Train stacking ensemble\n",
    "stacking_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate stacking ensemble\n",
    "stacking_results = evaluate_model(stacking_pipeline, X_test, y_test, \"Stacking Ensemble\", naive_mae)\n",
    "results.append(stacking_results)\n",
    "predictions['Stacking Ensemble'] = stacking_results['y_pred']\n",
    "\n",
    "# 7. Model Comparison\n",
    "\n",
    "print(\"\\nComparing all models...\")\n",
    "\n",
    "# Create a dataframe with all results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df[['model_name', 'mae', 'rmse', 'mape', 'r2']])\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['mae'].idxmin()\n",
    "best_model = results_df.loc[best_model_idx, 'model_name']\n",
    "print(f\"\\nBest model based on MAE: {best_model}\")\n",
    "\n",
    "# Calculate improvements over naive baseline\n",
    "results_df['improvement_over_naive'] = ((naive_mae - results_df['mae']) / naive_mae) * 100\n",
    "\n",
    "# Plot MAE comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model_name', y='mae', data=results_df)\n",
    "plt.title('MAE Comparison Across Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Absolute Error (MW)')\n",
    "plt.axhline(y=naive_mae, color='r', linestyle='--', label=f'Naive Baseline: {naive_mae:.2f}')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_mae_comparison.png')\n",
    "\n",
    "# Plot MAPE comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model_name', y='mape', data=results_df)\n",
    "plt.title('MAPE Comparison Across Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean Absolute Percentage Error (%)')\n",
    "plt.axhline(y=naive_mape, color='r', linestyle='--', label=f'Naive Baseline: {naive_mape:.2f}%')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_mape_comparison.png')\n",
    "\n",
    "# Plot R² comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model_name', y='r2', data=results_df)\n",
    "plt.title('R² Comparison Across Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('R² Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_r2_comparison.png')\n",
    "\n",
    "# Plot improvement over naive baseline\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model_name', y='improvement_over_naive', data=results_df)\n",
    "plt.title('Improvement Over Naive Baseline')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Improvement (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_improvement_comparison.png')\n",
    "\n",
    "# 8. Feature Importance Analysis\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "\n",
    "# Extract feature names from the preprocessor\n",
    "feature_names = []\n",
    "for name, transformer, features in preprocessor.transformers_:\n",
    "    if name == 'num':\n",
    "        feature_names.extend(features)\n",
    "    elif name == 'cat':\n",
    "        # For categorical features, get the one-hot encoded feature names\n",
    "        for cat_feature in features:\n",
    "            categories = train_df[cat_feature].unique()\n",
    "            for category in categories:\n",
    "                feature_names.append(f\"{cat_feature}_{category}\")\n",
    "\n",
    "# Get feature importance from Random Forest and XGBoost\n",
    "try:\n",
    "    # For Random Forest\n",
    "    rf_model = rf_pipeline.named_steps['model']\n",
    "    rf_importances = pd.DataFrame({\n",
    "        'feature': feature_names[:len(rf_model.feature_importances_)],\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=rf_importances)\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rf_feature_importance.png')\n",
    "\n",
    "    # For XGBoost\n",
    "    xgb_model = xgb_pipeline.named_steps['model']\n",
    "    xgb_importances = pd.DataFrame({\n",
    "        'feature': feature_names[:len(xgb_model.feature_importances_)],\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=xgb_importances)\n",
    "    plt.title('XGBoost Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('xgb_feature_importance.png')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in feature importance analysis: {e}\")\n",
    "\n",
    "# 9. Visualize predictions from all models in one plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(test_dates, y_test, 'k-', label='Actual', linewidth=2, alpha=0.7)\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown']\n",
    "color_idx = 0\n",
    "\n",
    "for model_name, y_pred in predictions.items():\n",
    "    if model_name == 'Naive Baseline':\n",
    "        plt.plot(test_dates, y_pred, 'r--', label=model_name, alpha=0.5)\n",
    "    else:\n",
    "        if color_idx < len(colors):\n",
    "            plt.plot(test_dates, y_pred, color=colors[color_idx], label=model_name, alpha=0.6)\n",
    "            color_idx += 1\n",
    "        else:\n",
    "            plt.plot(test_dates, y_pred, label=model_name, alpha=0.6)\n",
    "\n",
    "plt.title('Demand Predictions Comparison Across Models')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Demand (MW)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_models_prediction_comparison.png')\n",
    "\n",
    "# 10. Error Analysis\n",
    "print(\"\\nPerforming error analysis...\")\n",
    "\n",
    "# Choose the best model for error analysis\n",
    "best_model_name = best_model\n",
    "best_model_predictions = predictions[best_model_name]\n",
    "\n",
    "# Calculate errors\n",
    "errors = y_test - best_model_predictions\n",
    "abs_errors = np.abs(errors)\n",
    "rel_errors = abs_errors / y_test * 100\n",
    "\n",
    "# Create a DataFrame for error analysis\n",
    "error_df = pd.DataFrame({\n",
    "    'DateTime': test_dates,\n",
    "    'Hour': test_df['Hour'],\n",
    "    'Weekday': test_df['Weekday'],\n",
    "    'Month': test_df['Month'],\n",
    "    'Day': test_df['Day'],\n",
    "    'Season': test_df['Season'],\n",
    "    'Temperature (F)': test_df['Temperature (F)'],\n",
    "    'Actual': y_test,\n",
    "    'Predicted': best_model_predictions,\n",
    "    'Error': errors,\n",
    "    'AbsError': abs_errors,\n",
    "    'RelError': rel_errors\n",
    "})\n",
    "\n",
    "# Error distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(errors, kde=True, bins=50)\n",
    "plt.title(f'Distribution of Prediction Errors ({best_model_name})')\n",
    "plt.xlabel('Error (MW)')\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_distribution.png')\n",
    "\n",
    "# Error by hour of day\n",
    "hourly_error = error_df.groupby('Hour')['AbsError'].mean().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Hour', y='AbsError', data=hourly_error, marker='o')\n",
    "plt.title(f'Average Absolute Error by Hour of Day ({best_model_name})')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Mean Absolute Error (MW)')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_by_hour.png')\n",
    "\n",
    "# Error by day of week\n",
    "weekday_error = error_df.groupby('Weekday')['AbsError'].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Weekday', y='AbsError', data=weekday_error)\n",
    "plt.title(f'Average Absolute Error by Day of Week ({best_model_name})')\n",
    "plt.xlabel('Day of Week (0=Monday, 6=Sunday)')\n",
    "plt.ylabel('Mean Absolute Error (MW)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_by_weekday.png')\n",
    "\n",
    "# Error by temperature\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='Temperature (F)', y='AbsError', data=error_df, alpha=0.3)\n",
    "plt.title(f'Absolute Error vs Temperature ({best_model_name})')\n",
    "plt.xlabel('Temperature (F)')\n",
    "plt.ylabel('Absolute Error (MW)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('error_vs_temperature.png')\n",
    "\n",
    "# Relative error by demand level\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='Actual', y='RelError', data=error_df, alpha=0.3)\n",
    "plt.title(f'Relative Error vs Actual Demand ({best_model_name})')\n",
    "plt.xlabel('Actual Demand (MW)')\n",
    "plt.ylabel('Relative Error (%)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('relative_error_vs_demand.png')\n",
    "\n",
    "# 11. Next-Day Forecast Simulation (fixed version)\n",
    "print(\"\\nSimulating next-day hourly forecast...\")\n",
    "\n",
    "# Get the last day in the test set\n",
    "last_date = test_df['DateTime'].max()\n",
    "next_day = last_date + timedelta(days=1)\n",
    "print(f\"Simulating forecast for: {next_day.date()}\")\n",
    "\n",
    "# Create a template for next 24 hours\n",
    "next_day_hours = [next_day.replace(hour=h) for h in range(24)]\n",
    "next_day_df = pd.DataFrame({'DateTime': next_day_hours})\n",
    "\n",
    "# Add temporal features\n",
    "next_day_df['Hour'] = next_day_df['DateTime'].dt.hour\n",
    "next_day_df['Day'] = next_day_df['DateTime'].dt.day\n",
    "next_day_df['Month'] = next_day_df['DateTime'].dt.month\n",
    "next_day_df['Weekday'] = next_day_df['DateTime'].dt.weekday\n",
    "next_day_df['Weekend'] = next_day_df['Weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Determine season (simplistic approach)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 1  # Winter\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 2  # Spring\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 3  # Summer\n",
    "    else:\n",
    "        return 4  # Fall\n",
    "\n",
    "next_day_df['Season'] = next_day_df['Month'].apply(get_season)\n",
    "\n",
    "# For weather data, use the last known values (or you could use weather forecast data if available)\n",
    "\n",
    "weather_cols = ['Temperature (F)', 'Humidity', 'Wind Speed (mph)', 'Pressure', 'UV Index', 'Cloud Cover']\n",
    "last_day_weather = df_features[df_features['DateTime'] >= last_date - timedelta(days=1)]\n",
    "\n",
    "# Interpolate by hour if we have some data for the last day\n",
    "if len(last_day_weather) > 0:\n",
    "    for hour in range(24):\n",
    "        hour_data = last_day_weather[last_day_weather['Hour'] == hour]\n",
    "        if len(hour_data) > 0:\n",
    "            for col in weather_cols:\n",
    "                next_day_df.loc[next_day_df['Hour'] == hour, col] = hour_data[col].values[-1]\n",
    "        else:\n",
    "            \n",
    "            last_week = df_features[(df_features['DateTime'] >= last_date - timedelta(days=7)) &\n",
    "                                  (df_features['Hour'] == hour)]\n",
    "            for col in weather_cols:\n",
    "                next_day_df.loc[next_day_df['Hour'] == hour, col] = last_week[col].mean()\n",
    "\n",
    "# Add Sub-Region \n",
    "next_day_df['Sub-Region'] = test_df['Sub-Region'].iloc[-1]\n",
    "\n",
    "# Add lag features - only lag 24h\n",
    "next_day_df['demand_lag_24h'] = test_df['Demand (MW)'].iloc[-24:].values[0] if len(test_df) >= 24 else np.nan\n",
    "\n",
    "# Add demand lag for previous day same hour (24, 48, 72 hours)\n",
    "for i in range(1, 4):\n",
    "    for hour in range(24):\n",
    "        same_hour_data = test_df[(test_df['DateTime'] <= last_date) & (test_df['Hour'] == hour)]\n",
    "        if len(same_hour_data) >= i:\n",
    "            next_day_df.loc[next_day_df['Hour'] == hour, f'demand_lag_{i}d_same_hour'] = same_hour_data['Demand (MW)'].iloc[-i]\n",
    "        else:\n",
    "            next_day_df.loc[next_day_df['Hour'] == hour, f'demand_lag_{i}d_same_hour'] = np.nan\n",
    "\n",
    "# Add moving averages (simplified)\n",
    "last_24h = test_df['Demand (MW)'].iloc[-24:].mean() if len(test_df) >= 24 else test_df['Demand (MW)'].mean()\n",
    "last_7d = test_df['Demand (MW)'].iloc[-24*7:].mean() if len(test_df) >= 24*7 else test_df['Demand (MW)'].mean()\n",
    "next_day_df['demand_ma_24h'] = last_24h\n",
    "next_day_df['demand_ma_7d'] = last_7d\n",
    "\n",
    "# Add daily stats (simplified)\n",
    "last_day_stats = test_df.groupby(test_df['DateTime'].dt.date)['Demand (MW)'].agg(['min', 'max'])\n",
    "if not last_day_stats.empty:\n",
    "    next_day_df['daily_min'] = last_day_stats['min'].iloc[-1]\n",
    "    next_day_df['daily_max'] = last_day_stats['max'].iloc[-1]\n",
    "    next_day_df['daily_range'] = next_day_df['daily_max'] - next_day_df['daily_min']\n",
    "else:\n",
    "    next_day_df['daily_min'] = test_df['Demand (MW)'].min()\n",
    "    next_day_df['daily_max'] = test_df['Demand (MW)'].max()\n",
    "    next_day_df['daily_range'] = next_day_df['daily_max'] - next_day_df['daily_min']\n",
    "\n",
    "# Add Cyclical Features\n",
    "next_day_df['hour_sin'] = np.sin(2 * np.pi * next_day_df['Hour']/24)\n",
    "next_day_df['hour_cos'] = np.cos(2 * np.pi * next_day_df['Hour']/24)\n",
    "next_day_df['weekday_sin'] = np.sin(2 * np.pi * next_day_df['Weekday']/7)\n",
    "next_day_df['weekday_cos'] = np.cos(2 * np.pi * next_day_df['Weekday']/7)\n",
    "next_day_df['month_sin'] = np.sin(2 * np.pi * next_day_df['Month']/12)\n",
    "next_day_df['month_cos'] = np.cos(2 * np.pi * next_day_df['Month']/12)\n",
    "\n",
    "# Temperature change (simplified)\n",
    "next_day_df['temp_change_24h'] = 0  # Placeholder\n",
    "\n",
    "# Create peak/off-peak indicator\n",
    "next_day_df['is_peak_hours'] = ((next_day_df['Hour'] >= 6) & (next_day_df['Hour'] <= 22)).astype(int)\n",
    "\n",
    "# Fill missing numerical and categorical values in X_next_day\n",
    "for col in numerical_features:\n",
    "    if col in next_day_df.columns:\n",
    "        next_day_df[col] = next_day_df[col].fillna(X_train[col].median())\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in next_day_df.columns:\n",
    "        next_day_df[col] = next_day_df[col].fillna(X_train[col].mode()[0])\n",
    "\n",
    "# Extract features needed for prediction \n",
    "X_next_day = next_day_df[all_features]\n",
    "\n",
    "# Map model names to actual model objects\n",
    "model_map = {\n",
    "    \"Linear Regression\": lr_pipeline,\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"Gradient Boosting\": gb_pipeline,\n",
    "    \"XGBoost\": xgb_pipeline,\n",
    "    \"Stacking Ensemble\": stacking_pipeline\n",
    "}\n",
    "\n",
    "# Add Neural Network if available\n",
    "if 'nn_model' in locals() and 'nn_pred' in locals():\n",
    "    model_map[\"Neural Network\"] = nn_model\n",
    "\n",
    "# Add SARIMA if available\n",
    "if 'sarima_results' in locals() and 'sarima_pred' in locals():\n",
    "    model_map[\"SARIMA\"] = sarima_results\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = results_df.loc[results_df['mae'].idxmin(), 'model_name']\n",
    "best_model = model_map.get(best_model_name)\n",
    "\n",
    "if best_model is None:\n",
    "    raise ValueError(f\"Best model '{best_model_name}' is not available or not trained.\")\n",
    "\n",
    "# For Neural Network, we need to transform the data differently\n",
    "if best_model_name == \"Neural Network\":\n",
    "    X_next_day_transformed = preprocessor.transform(X_next_day)\n",
    "    next_day_pred = best_model.predict(X_next_day_transformed).flatten()\n",
    "elif best_model_name == \"SARIMA\":\n",
    "    # SARIMA forecast logic would go here\n",
    "    print(\"SARIMA forecast requires special handling...\")\n",
    "    \n",
    "else:\n",
    "    # For scikit-learn models, we can use the pipeline directly\n",
    "    next_day_pred = best_model.predict(X_next_day)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "next_day_df['Predicted_Demand'] = next_day_pred\n",
    "\n",
    "# Plot next day forecast\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(next_day_df['DateTime'], next_day_df['Predicted_Demand'], 'b-', marker='o', label=f'Forecast ({best_model_name})')\n",
    "plt.title(f'Next-Day Hourly Demand Forecast for {next_day.date()}')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Demand (MW)')\n",
    "plt.xticks(next_day_df['DateTime'], [f\"{h}:00\" for h in range(24)], rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('next_day_forecast.png')\n",
    "\n",
    "# 12. Optional: Time Series Models (SARIMA)\n",
    "\n",
    "try:\n",
    "    print(\"\\nTraining SARIMA model (this may take some time)...\")\n",
    "\n",
    "    # Prepare time series data for SARIMA\n",
    "    \n",
    "    ts_data = df_features[['DateTime', 'Demand (MW)']].set_index('DateTime')\n",
    "    ts_data = ts_data.resample('H').mean()  # Ensure hourly frequency\n",
    "\n",
    "    # Split into train/test\n",
    "    ts_train = ts_data[ts_data.index <= split_date]\n",
    "    ts_test = ts_data[ts_data.index > split_date]\n",
    "\n",
    "    # Fit SARIMA model \n",
    "    \n",
    "    sarima_model = SARIMAX(ts_train,\n",
    "                           order=(1, 1, 1),\n",
    "                           seasonal_order=(1, 1, 1, 24),  # 24-hour seasonality\n",
    "                           enforce_stationarity=False,\n",
    "                           enforce_invertibility=False)\n",
    "\n",
    "    sarima_results = sarima_model.fit(disp=False)\n",
    "    print(\"SARIMA model summary:\")\n",
    "    print(sarima_results.summary())\n",
    "\n",
    "    # Make predictions\n",
    "    sarima_pred = sarima_results.forecast(steps=len(ts_test))\n",
    "\n",
    "    # Evaluate\n",
    "    sarima_mae = mean_absolute_error(ts_test, sarima_pred)\n",
    "    sarima_rmse = np.sqrt(mean_squared_error(ts_test, sarima_pred))\n",
    "    sarima_mape = np.mean(np.abs((ts_test.values - sarima_pred) / ts_test.values)) * 100\n",
    "\n",
    "    print(f\"\\nSARIMA Model Performance:\")\n",
    "    print(f\"MAE: {sarima_mae:.2f} MW\")\n",
    "    print(f\"RMSE: {sarima_rmse:.2f} MW\")\n",
    "    print(f\"MAPE: {sarima_mape:.2f}%\")\n",
    "\n",
    "    # Calculate improvement over baseline\n",
    "    sarima_improvement = ((naive_mae - sarima_mae) / naive_mae) * 100\n",
    "    print(f\"Improvement over naive baseline: {sarima_improvement:.2f}%\")\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(ts_test.index, ts_test.values, label='Actual Demand', alpha=0.7)\n",
    "    plt.plot(ts_test.index, sarima_pred, label='SARIMA Prediction', alpha=0.7)\n",
    "    plt.title('SARIMA: Actual vs Predicted Demand')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand (MW)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sarima_prediction.png')\n",
    "\n",
    "    # Add to results\n",
    "    sarima_results_dict = {\n",
    "        'model_name': 'SARIMA',\n",
    "        'mae': sarima_mae,\n",
    "        'rmse': sarima_rmse,\n",
    "        'mape': sarima_mape,\n",
    "        'r2': 0,  # R² not commonly used for time series\n",
    "        'y_pred': sarima_pred,\n",
    "        'improvement_over_naive': sarima_improvement\n",
    "    }\n",
    "    results.append(sarima_results_dict)\n",
    "    predictions['SARIMA'] = sarima_pred\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error training SARIMA model: {e}\")\n",
    "    pass\n",
    "\n",
    "# 13. Conclusions \n",
    "\n",
    "print(\"\\n==== Electricity Demand Forecasting Project Summary ====\")\n",
    "print(f\"\\nTotal observations: {len(df)}\")\n",
    "print(f\"Time period: {df['DateTime'].min()} to {df['DateTime'].max()}\")\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "\n",
    "# Update results \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by MAE for final comparison\n",
    "final_comparison = results_df.sort_values('mae')[['model_name', 'mae', 'rmse', 'mape', 'r2']]\n",
    "print(\"\\nFinal Model Comparison (sorted by MAE):\")\n",
    "print(final_comparison)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n==== End of Project Summary ====\")\n",
    "\n",
    "print(\"\\nAll analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save Linear Regression\n",
    "joblib.dump(lr_model, 'linear_regression_model.pkl')\n",
    "\n",
    "# Save Ridge Regression\n",
    "joblib.dump(ridge_model, 'ridge_model.pkl')\n",
    "\n",
    "# Save Random Forest\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "\n",
    "# Save Gradient Boosting\n",
    "joblib.dump(gb_model, 'gradient_boosting_model.pkl')\n",
    "\n",
    "# Save XGBoost\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "\n",
    "# Save Stacking Regressor\n",
    "joblib.dump(stacking_model, 'stacking_model.pkl')\n",
    "\n",
    "# Save LSTM Model\n",
    "lstm_model.save('lstm_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
