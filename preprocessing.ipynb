{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26775794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files combined and saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV files to combine\n",
    "csv_files = [\n",
    "    'EIA930_SUBREGION_2018_Jul_Dec.csv',\n",
    "    'EIA930_SUBREGION_2019_Jan_Jun.csv',\n",
    "    'EIA930_SUBREGION_2019_Jul_Dec.csv',\n",
    "    'EIA930_SUBREGION_2020_Jan_Jun.csv'\n",
    "]\n",
    "\n",
    "# Read and concatenate all CSVs\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save to a new CSV\n",
    "combined_df.to_csv('merged_region.csv', index=False)\n",
    "\n",
    "print(\"CSV files combined and saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e927d59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leads trading\\AppData\\Local\\Temp\\ipykernel_13704\\1745256236.py:7: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Demand (MW)'] = df['Demand (MW)'].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'Demand (MW)' filled using forward fill and file updated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('merged_region.csv')\n",
    "\n",
    "# Fill missing values in 'Demand (MW)' using forward fill\n",
    "df['Demand (MW)'] = df['Demand (MW)'].fillna(method='ffill')\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv('merged_region.csv', index=False)\n",
    "\n",
    "print(\"Missing values in 'Demand (MW)' filled using forward fill and file updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "285c8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1355952 entries, 0 to 1355951\n",
      "Data columns (total 7 columns):\n",
      " #   Column                     Non-Null Count    Dtype \n",
      "---  ------                     --------------    ----- \n",
      " 0   Balancing Authority        1355952 non-null  object\n",
      " 1   Data Date                  1355952 non-null  object\n",
      " 2   Hour Number                1355952 non-null  int64 \n",
      " 3   Sub-Region                 1355952 non-null  object\n",
      " 4   Demand (MW)                1355952 non-null  object\n",
      " 5   Local Time at End of Hour  1355952 non-null  object\n",
      " 6   UTC Time at End of Hour    1355952 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 72.4+ MB\n",
      "None\n",
      "Balancing Authority          0\n",
      "Data Date                    0\n",
      "Hour Number                  0\n",
      "Sub-Region                   0\n",
      "Demand (MW)                  0\n",
      "Local Time at End of Hour    0\n",
      "UTC Time at End of Hour      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('merged_region.csv')\n",
    "\n",
    "\n",
    "print(df.info())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c170743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Sub-Regions:\n",
      "PGAE\n",
      "SCE\n",
      "SDGE\n",
      "VEA\n",
      "COAS\n",
      "EAST\n",
      "FWES\n",
      "NCEN\n",
      "NRTH\n",
      "SCEN\n",
      "SOUT\n",
      "WEST\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "0001\n",
      "0004\n",
      "0006\n",
      "0027\n",
      "0035\n",
      "8910\n",
      "ZONA\n",
      "ZONB\n",
      "ZONC\n",
      "ZOND\n",
      "ZONE\n",
      "ZONF\n",
      "ZONG\n",
      "ZONH\n",
      "ZONI\n",
      "ZONJ\n",
      "ZONK\n",
      "AE\n",
      "AEP\n",
      "AP\n",
      "ATSI\n",
      "BC\n",
      "CE\n",
      "DAY\n",
      "DEOK\n",
      "DOM\n",
      "DPL\n",
      "DUQ\n",
      "EKPC\n",
      "JC\n",
      "ME\n",
      "PE\n",
      "PEP\n",
      "PL\n",
      "PN\n",
      "PS\n",
      "RECO\n",
      "Frep\n",
      "Jica\n",
      "KAFB\n",
      "KCEC\n",
      "LAC\n",
      "NTUA\n",
      "PNM\n",
      "TSGT\n",
      "CSWS\n",
      "EDE\n",
      "GRDA\n",
      "INDN\n",
      "KACY\n",
      "KCPL\n",
      "LES\n",
      "MPS\n",
      "NPPD\n",
      "OKGE\n",
      "OPPD\n",
      "SECI\n",
      "SPRM\n",
      "SPS\n",
      "WAUE\n",
      "WFEC\n",
      "WR\n",
      "1\n",
      "4\n",
      "6\n",
      "27\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "merged_region_df = pd.read_csv('merged_region.csv')\n",
    "\n",
    "# Get the unique sub-regions\n",
    "unique_subregions = merged_region_df['Sub-Region'].unique()\n",
    "\n",
    "# Print the unique sub-regions\n",
    "print('Unique Sub-Regions:')\n",
    "for subregion in unique_subregions:\n",
    "    print(subregion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f1ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched la with BA=CISO, Sub-Region=SCE\n",
      "Matched san_diego with BA=CISO, Sub-Region=SDGE\n",
      "Matched san_jose with BA=CISO, Sub-Region=PGAE\n",
      "Matched dallas with BA=ERCO, Sub-Region=NRTH\n",
      "Matched houston with BA=ERCO, Sub-Region=SCEN\n",
      "Matched san_antonio with BA=ERCO, Sub-Region=SOUT\n",
      "Matched nyc with BA=NYIS, Sub-Region=ZONA\n",
      "Matched philadelphia with BA=PJM, Sub-Region=PE\n",
      "Matched phoenix with BA=PNM, Sub-Region=PNM\n",
      "Matched seattle with BA=PJM, Sub-Region=PS\n",
      "\n",
      "Successfully created dataset with 165202 rows\n",
      "Cities included: ['la' 'san_diego' 'san_jose' 'dallas' 'houston' 'san_antonio' 'nyc'\n",
      " 'philadelphia' 'phoenix' 'seattle']\n",
      "\n",
      "Rows per city:\n",
      "City\n",
      "la              16531\n",
      "san_diego       16531\n",
      "san_jose        16531\n",
      "phoenix         16531\n",
      "seattle         16531\n",
      "nyc             16510\n",
      "philadelphia    16510\n",
      "dallas          16509\n",
      "houston         16509\n",
      "san_antonio     16509\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and clean the demand data\n",
    "merged_region = pd.read_csv('merged_region.csv')\n",
    "merged_region['Demand (MW)'] = merged_region['Demand (MW)'].str.replace(',', '').astype(float)\n",
    "merged_region['Local Time at End of Hour'] = pd.to_datetime(\n",
    "    merged_region['Local Time at End of Hour'], \n",
    "    format='mixed', \n",
    "    errors='coerce'\n",
    ")\n",
    "merged_region = merged_region.dropna(subset=['Local Time at End of Hour'])\n",
    "\n",
    "# Step 2: Load all weather data\n",
    "weather_files = [\n",
    "    'dallas.json', 'houston.json', 'la.json', 'nyc.json', 'philadelphia.json',\n",
    "    'phoenix.json', 'san_antonio.json', 'san_diego.json', 'san_jose.json', 'seattle.json'\n",
    "]\n",
    "\n",
    "weather_data = {}\n",
    "for file in weather_files:\n",
    "    city_name = file.split('.')[0]\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            weather_data[city_name] = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File {file} not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "# Step 3: Create weather DataFrames\n",
    "weather_dfs = {}\n",
    "for city, data in weather_data.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df['datetime'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df['city'] = city\n",
    "    weather_dfs[city] = df\n",
    "\n",
    "# Step 4: Define city mappings with multiple options for Seattle\n",
    "city_mappings = {\n",
    "    # California\n",
    "    'la': [('CISO', 'SCE')],\n",
    "    'san_diego': [('CISO', 'SDGE')],\n",
    "    'san_jose': [('CISO', 'PGAE')],\n",
    "    \n",
    "    # Texas\n",
    "    'dallas': [('ERCO', 'NRTH')],\n",
    "    'houston': [('ERCO', 'SCEN')],\n",
    "    'san_antonio': [('ERCO', 'SOUT')],\n",
    "    \n",
    "    # Northeast\n",
    "    'nyc': [('NYIS', 'ZONA'), ('NYIS', '0001')],\n",
    "    'philadelphia': [('PJM', 'PE')],\n",
    "    \n",
    "    # Southwest\n",
    "    'phoenix': [('PNM', 'PNM'), ('AZPS', 'AZPS')],\n",
    "    \n",
    "    # Northwest - multiple options for Seattle\n",
    "    'seattle': [\n",
    "        ('MISO', 'WEST'),\n",
    "        ('NWPP', 'WAUW'),\n",
    "        ('BPAT', 'WAUE'),\n",
    "        ('PJM', 'PS')\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Step 5: Match cities with demand data\n",
    "matched_data = []\n",
    "\n",
    "for city, ba_subregion_options in city_mappings.items():\n",
    "    if city not in weather_dfs:\n",
    "        print(f\"Weather data not found for {city}\")\n",
    "        continue\n",
    "        \n",
    "    weather_df = weather_dfs[city]\n",
    "    matched_for_city = False\n",
    "    \n",
    "    for ba, sub_region in ba_subregion_options:\n",
    "        matching_demand = merged_region[\n",
    "            (merged_region['Balancing Authority'] == ba) & \n",
    "            (merged_region['Sub-Region'] == sub_region)\n",
    "        ]\n",
    "        \n",
    "        if matching_demand.empty:\n",
    "            continue\n",
    "            \n",
    "        # Found matching demand data\n",
    "        matched_for_city = True\n",
    "        print(f\"Matched {city} with BA={ba}, Sub-Region={sub_region}\")\n",
    "        \n",
    "        for _, demand_row in matching_demand.iterrows():\n",
    "            local_time = demand_row['Local Time at End of Hour']\n",
    "            time_diff = (weather_df['datetime'] - local_time).abs()\n",
    "            closest_idx = time_diff.idxmin()\n",
    "            closest_time_diff = time_diff[closest_idx].total_seconds() / 3600\n",
    "            \n",
    "            if closest_time_diff <= 1:\n",
    "                weather_row = weather_df.loc[closest_idx]\n",
    "                \n",
    "                merged_row = {\n",
    "                    'Balancing Authority': ba,\n",
    "                    'Data Date': demand_row['Data Date'],\n",
    "                    'Hour Number': demand_row['Hour Number'],\n",
    "                    'Sub-Region': sub_region,\n",
    "                    'Demand (MW)': demand_row['Demand (MW)'],\n",
    "                    'Local Time': local_time,\n",
    "                    'City': city,\n",
    "                    'Temperature (F)': weather_row['temperature'],\n",
    "                    'Humidity': weather_row.get('humidity', np.nan),\n",
    "                    'Wind Speed (mph)': weather_row.get('windSpeed', np.nan),\n",
    "                    'Weather Summary': weather_row.get('summary', ''),\n",
    "                    'Pressure': weather_row.get('pressure', np.nan),\n",
    "                    'UV Index': weather_row.get('uvIndex', np.nan),\n",
    "                    'Visibility': weather_row.get('visibility', np.nan),\n",
    "                    'Cloud Cover': weather_row.get('cloudCover', np.nan)\n",
    "                }\n",
    "                \n",
    "                matched_data.append(merged_row)\n",
    "        \n",
    "        break  # Stop after first successful match\n",
    "    \n",
    "    if not matched_for_city:\n",
    "        print(f\"Could not match {city} with any BA/Sub-Region combination\")\n",
    "\n",
    "# Step 6: Create final DataFrame and add features\n",
    "final_df = pd.DataFrame(matched_data)\n",
    "\n",
    "if not final_df.empty:\n",
    "    # Add time-based features\n",
    "    final_df['Hour'] = final_df['Local Time'].dt.hour\n",
    "    final_df['Day'] = final_df['Local Time'].dt.day\n",
    "    final_df['Month'] = final_df['Local Time'].dt.month\n",
    "    final_df['Weekday'] = final_df['Local Time'].dt.dayofweek\n",
    "    final_df['Weekend'] = final_df['Weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    final_df['Season'] = final_df['Month'].apply(\n",
    "        lambda x: 1 if x in [12,1,2] else 2 if x in [3,4,5] else 3 if x in [6,7,8] else 4\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv('merged_weather_demand_final.csv', index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSuccessfully created dataset with\", len(final_df), \"rows\")\n",
    "    print(\"Cities included:\", final_df['City'].unique())\n",
    "    print(\"\\nRows per city:\")\n",
    "    print(final_df['City'].value_counts())\n",
    "else:\n",
    "    print(\"No matching data was found. Please check your data files and mappings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241a58e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Missing Values Per Column:\n",
      "==================================================\n",
      "Temperature (F)      10\n",
      "Humidity             10\n",
      "Wind Speed (mph)     53\n",
      "Weather Summary     277\n",
      "Pressure             50\n",
      "UV Index            175\n",
      "Visibility          172\n",
      "Cloud Cover         240\n",
      "dtype: int64\n",
      "\n",
      "==================================================\n",
      "Missing Values Percentage:\n",
      "==================================================\n",
      "Temperature (F)     0.01\n",
      "Humidity            0.01\n",
      "Wind Speed (mph)    0.03\n",
      "Weather Summary     0.17\n",
      "Pressure            0.03\n",
      "UV Index            0.11\n",
      "Visibility          0.10\n",
      "Cloud Cover         0.15\n",
      "dtype: float64\n",
      "\n",
      "==================================================\n",
      "Complete Cases Analysis:\n",
      "==================================================\n",
      "Total rows: 165202\n",
      "Complete rows (no missing values): 164869\n",
      "Percentage complete: 99.80%\n",
      "\n",
      "==================================================\n",
      "Missing Values By City:\n",
      "==================================================\n",
      "la: 98 missing values\n",
      "san_diego: 99 missing values\n",
      "san_jose: 99 missing values\n",
      "dallas: 99 missing values\n",
      "houston: 100 missing values\n",
      "san_antonio: 100 missing values\n",
      "nyc: 97 missing values\n",
      "philadelphia: 96 missing values\n",
      "phoenix: 98 missing values\n",
      "seattle: 101 missing values\n",
      "\n",
      "==================================================\n",
      "Detailed Missing Data Pattern:\n",
      "==================================================\n",
      "Columns with missing values and sample affected rows:\n",
      "\n",
      "Column: Temperature (F)\n",
      "Missing count: 10\n",
      "Sample rows with missing values:\n",
      "            City           Local Time  Temperature (F)\n",
      "413           la  2018-07-18 11:00:00              NaN\n",
      "16944  san_diego  2018-07-18 11:00:00              NaN\n",
      "33475   san_jose  2018-07-18 11:00:00              NaN\n",
      "\n",
      "Column: Humidity\n",
      "Missing count: 10\n",
      "Sample rows with missing values:\n",
      "            City           Local Time  Humidity\n",
      "413           la  2018-07-18 11:00:00       NaN\n",
      "16944  san_diego  2018-07-18 11:00:00       NaN\n",
      "33475   san_jose  2018-07-18 11:00:00       NaN\n",
      "\n",
      "Column: Wind Speed (mph)\n",
      "Missing count: 53\n",
      "Sample rows with missing values:\n",
      "    City           Local Time  Wind Speed (mph)\n",
      "380   la  2018-07-17 02:00:00               NaN\n",
      "413   la  2018-07-18 11:00:00               NaN\n",
      "470   la  2018-07-20 20:00:00               NaN\n",
      "\n",
      "Column: Weather Summary\n",
      "Missing count: 277\n",
      "Sample rows with missing values:\n",
      "    City           Local Time Weather Summary\n",
      "375   la  2018-07-16 21:00:00             NaN\n",
      "380   la  2018-07-17 02:00:00             NaN\n",
      "381   la  2018-07-17 03:00:00             NaN\n",
      "\n",
      "Column: Pressure\n",
      "Missing count: 50\n",
      "Sample rows with missing values:\n",
      "     City           Local Time  Pressure\n",
      "599    la  2018-07-26 05:00:00       NaN\n",
      "621    la  2018-07-27 03:00:00       NaN\n",
      "1224   la  2018-08-21 06:00:00       NaN\n",
      "\n",
      "Column: UV Index\n",
      "Missing count: 175\n",
      "Sample rows with missing values:\n",
      "    City           Local Time  UV Index\n",
      "375   la  2018-07-16 21:00:00       NaN\n",
      "380   la  2018-07-17 02:00:00       NaN\n",
      "381   la  2018-07-17 03:00:00       NaN\n",
      "\n",
      "Column: Visibility\n",
      "Missing count: 172\n",
      "Sample rows with missing values:\n",
      "    City           Local Time  Visibility\n",
      "380   la  2018-07-17 02:00:00         NaN\n",
      "398   la  2018-07-17 20:00:00         NaN\n",
      "409   la  2018-07-18 07:00:00         NaN\n",
      "\n",
      "Column: Cloud Cover\n",
      "Missing count: 240\n",
      "Sample rows with missing values:\n",
      "    City           Local Time  Cloud Cover\n",
      "375   la  2018-07-16 21:00:00          NaN\n",
      "380   la  2018-07-17 02:00:00          NaN\n",
      "381   la  2018-07-17 03:00:00          NaN\n",
      "\n",
      "For visualizations, install missingno: pip install missingno\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged dataset\n",
    "df = pd.read_csv('merged_weather_demand_final.csv')\n",
    "\n",
    "# 1. Check for missing values in each column\n",
    "print(\"=\"*50)\n",
    "print(\"Missing Values Per Column:\")\n",
    "print(\"=\"*50)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])  # Only show columns with missing values\n",
    "\n",
    "# 2. Check percentage of missing values\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Missing Values Percentage:\")\n",
    "print(\"=\"*50)\n",
    "missing_percentage = (df.isnull().mean() * 100).round(2)\n",
    "print(missing_percentage[missing_percentage > 0])  # Only show columns with missing values\n",
    "\n",
    "# 3. Check for complete cases (rows with no missing values)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Complete Cases Analysis:\")\n",
    "print(\"=\"*50)\n",
    "complete_cases = df.dropna()\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Complete rows (no missing values): {len(complete_cases)}\")\n",
    "print(f\"Percentage complete: {len(complete_cases)/len(df)*100:.2f}%\")\n",
    "\n",
    "# 4. Check for missing values by city\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Missing Values By City:\")\n",
    "print(\"=\"*50)\n",
    "if 'City' in df.columns:\n",
    "    for city in df['City'].unique():\n",
    "        city_df = df[df['City'] == city]\n",
    "        city_missing = city_df.isnull().sum().sum()\n",
    "        print(f\"{city}: {city_missing} missing values\")\n",
    "\n",
    "# 5. Detailed missing data pattern\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Detailed Missing Data Pattern:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Columns with missing values and sample affected rows:\")\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().any():\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(f\"Missing count: {df[col].isnull().sum()}\")\n",
    "        print(\"Sample rows with missing values:\")\n",
    "        print(df[df[col].isnull()][['City', 'Local Time', col]].head(3))\n",
    "\n",
    "# Optional: Visualize missing data (requires matplotlib)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import missingno as msno\n",
    "    \n",
    "    print(\"\\nGenerating missing data visualization...\")\n",
    "    msno.matrix(df)\n",
    "    plt.title('Missing Data Pattern')\n",
    "    plt.show()\n",
    "    \n",
    "    msno.bar(df)\n",
    "    plt.title('Missing Values Per Column')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"\\nFor visualizations, install missingno: pip install missingno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dcf5413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leads trading\\AppData\\Local\\Temp\\ipykernel_14780\\2189600972.py:17: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Weather Summary'] = df['Weather Summary'].fillna(method='ffill')\n",
      "C:\\Users\\leads trading\\AppData\\Local\\Temp\\ipykernel_14780\\2189600972.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Missing values handled using time-aware methods and file saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset and parse 'Local Time' as datetime\n",
    "df = pd.read_csv('merged_weather_demand_final.csv', parse_dates=['Local Time'])\n",
    "\n",
    "# Set 'Local Time' as the datetime index for time-aware operations\n",
    "df.set_index('Local Time', inplace=True)\n",
    "\n",
    "# List of numeric columns to interpolate\n",
    "num_cols = ['Temperature (F)', 'Humidity', 'Wind Speed (mph)', \n",
    "            'Pressure', 'UV Index', 'Visibility', 'Cloud Cover']\n",
    "\n",
    "# Time-based interpolation for numeric values\n",
    "df[num_cols] = df[num_cols].interpolate(method='time')\n",
    "\n",
    "# Forward fill for the categorical column\n",
    "df['Weather Summary'] = df['Weather Summary'].fillna(method='ffill')\n",
    "\n",
    "# Backfill any remaining missing values (just in case)\n",
    "df = df.fillna(method='bfill')\n",
    "\n",
    "# Reset index to save 'Local Time' back as a column\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Save the cleaned dataset (overwrite the file)\n",
    "df.to_csv('merged_weather_demand_final.csv', index=False)\n",
    "\n",
    "print(\"✅ Missing values handled using time-aware methods and file saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df751d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('merged_weather_demand_final.csv')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Print columns with missing values (if any)\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be97511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "Dataset shape: (165202, 21)\n",
      "First few rows:\n",
      "            Local Time Balancing Authority   Data Date  Hour Number  \\\n",
      "0  2018-07-01 06:00:00                CISO  07/01/2018            6   \n",
      "1  2018-07-01 07:00:00                CISO  07/01/2018            7   \n",
      "\n",
      "  Sub-Region  Demand (MW) City  Temperature (F)  Humidity  Wind Speed (mph)  \\\n",
      "0        SCE       9472.0   la            65.45      0.79              4.23   \n",
      "1        SCE       9353.0   la            65.45      0.79              4.23   \n",
      "\n",
      "   ... Pressure  UV Index  Visibility  Cloud Cover  Hour  Day  Month  Weekday  \\\n",
      "0  ...   1014.5       0.0       9.798         0.25     6    1      7        6   \n",
      "1  ...   1014.5       0.0       9.798         0.25     7    1      7        6   \n",
      "\n",
      "   Weekend  Season  \n",
      "0        1       3  \n",
      "1        1       3  \n",
      "\n",
      "[2 rows x 21 columns]\n",
      "\n",
      "Converting date and time columns...\n",
      "DateTime conversion successful: True\n",
      "Sample DateTime values: [Timestamp('2018-07-01 06:00:00'), Timestamp('2018-07-01 07:00:00'), Timestamp('2018-07-01 08:00:00')]\n",
      "\n",
      "==== CHECKING TIME INTERVAL CONSISTENCY ====\n",
      "Time interval consistency by city:\n",
      "\n",
      "la:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 688 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "san_diego:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 688 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "san_jose:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 688 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "dallas:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 687 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "houston:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 687 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "san_antonio:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 687 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "nyc:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 687 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "philadelphia:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 687 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "phoenix:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 688 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "seattle:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 688 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "==== CHECKING FOR DUPLICATES ====\n",
      "Found 6885 duplicate records:\n",
      "                Local Time Balancing Authority   Data Date  Hour Number  \\\n",
      "49613  2018-07-03 00:00:00                ERCO  07/02/2018           24   \n",
      "49637  2018-07-04 00:00:00                ERCO  07/03/2018           24   \n",
      "49661  2018-07-05 00:00:00                ERCO  07/04/2018           24   \n",
      "49685  2018-07-06 00:00:00                ERCO  07/05/2018           24   \n",
      "49709  2018-07-07 00:00:00                ERCO  07/06/2018           24   \n",
      "49733  2018-07-08 00:00:00                ERCO  07/07/2018           24   \n",
      "49757  2018-07-09 00:00:00                ERCO  07/08/2018           24   \n",
      "49781  2018-07-10 00:00:00                ERCO  07/09/2018           24   \n",
      "49805  2018-07-11 00:00:00                ERCO  07/10/2018           24   \n",
      "49829  2018-07-12 00:00:00                ERCO  07/11/2018           24   \n",
      "\n",
      "      Sub-Region  Demand (MW)    City  Temperature (F)  Humidity  \\\n",
      "49613       NRTH         92.0  dallas           101.04      0.26   \n",
      "49637       NRTH         92.0  dallas           102.31      0.21   \n",
      "49661       NRTH         92.0  dallas            90.43      0.48   \n",
      "49685       NRTH         92.0  dallas            96.14      0.37   \n",
      "49709       NRTH         92.0  dallas            82.98      0.68   \n",
      "49733       NRTH         92.0  dallas            94.37      0.39   \n",
      "49757       NRTH         92.0  dallas            88.10      0.55   \n",
      "49781       NRTH         92.0  dallas            84.20      0.67   \n",
      "49805       NRTH         92.0  dallas            82.03      0.73   \n",
      "49829       NRTH         92.0  dallas            94.47      0.41   \n",
      "\n",
      "       Wind Speed (mph)  ... UV Index  Visibility  Cloud Cover  Hour  Day  \\\n",
      "49613              9.88  ...      1.0       9.997         0.00     0    3   \n",
      "49637              6.93  ...      1.0       9.997         0.06     0    4   \n",
      "49661             14.47  ...      1.0       9.997         0.79     0    5   \n",
      "49685              8.27  ...      1.0       9.828         0.42     0    6   \n",
      "49709              5.88  ...      1.0       9.806         0.22     0    7   \n",
      "49733             11.66  ...      1.0       9.997         0.41     0    8   \n",
      "49757             10.18  ...      1.0       9.997         0.52     0    9   \n",
      "49781              9.57  ...      1.0       9.795         0.80     0   10   \n",
      "49805              3.94  ...      1.0       9.997         0.76     0   11   \n",
      "49829              6.87  ...      1.0       9.997         0.23     0   12   \n",
      "\n",
      "       Month  Weekday  Weekend  Season  DateTime  \n",
      "49613      7        1        0       3       NaT  \n",
      "49637      7        2        0       3       NaT  \n",
      "49661      7        3        0       3       NaT  \n",
      "49685      7        4        0       3       NaT  \n",
      "49709      7        5        1       3       NaT  \n",
      "49733      7        6        1       3       NaT  \n",
      "49757      7        0        0       3       NaT  \n",
      "49781      7        1        0       3       NaT  \n",
      "49805      7        2        0       3       NaT  \n",
      "49829      7        3        0       3       NaT  \n",
      "\n",
      "[10 rows x 22 columns]\n",
      "\n",
      "Duplicates by city:\n",
      "City\n",
      "dallas          688\n",
      "houston         688\n",
      "la              689\n",
      "nyc             688\n",
      "philadelphia    688\n",
      "phoenix         689\n",
      "san_antonio     688\n",
      "san_diego       689\n",
      "san_jose        689\n",
      "seattle         689\n",
      "dtype: int64\n",
      "\n",
      "==== DETECTING ANOMALIES ====\n",
      "Detecting demand anomalies (using Z-score method)...\n",
      "Detecting weather anomalies (using IQR method)...\n",
      "Checking for physically impossible values...\n",
      "\n",
      "Found 1293 demand anomalies\n",
      "Demand anomalies by city:\n",
      "  la: 362 anomalies\n",
      "  san_diego: 231 anomalies\n",
      "  san_jose: 231 anomalies\n",
      "  dallas: 0 anomalies\n",
      "  houston: 0 anomalies\n",
      "  san_antonio: 0 anomalies\n",
      "  nyc: 163 anomalies\n",
      "  philadelphia: 0 anomalies\n",
      "  phoenix: 293 anomalies\n",
      "  seattle: 13 anomalies\n",
      "\n",
      "Sample demand anomalies:\n",
      "    City            DateTime  Demand (MW)  demand_zscore\n",
      "127   la 2018-07-06 13:00:00      20313.0       3.145769\n",
      "128   la 2018-07-06 14:00:00      21828.0       3.683887\n",
      "129   la 2018-07-06 15:00:00      22980.0       4.093069\n",
      "130   la 2018-07-06 16:00:00      23728.0       4.358754\n",
      "131   la 2018-07-06 17:00:00      24061.0       4.477033\n",
      "\n",
      "Found 6805 weather anomalies\n",
      "Weather anomalies by city:\n",
      "  la: 526 anomalies\n",
      "  san_diego: 1107 anomalies\n",
      "  san_jose: 666 anomalies\n",
      "  dallas: 475 anomalies\n",
      "  houston: 588 anomalies\n",
      "  san_antonio: 467 anomalies\n",
      "  nyc: 433 anomalies\n",
      "  philadelphia: 569 anomalies\n",
      "  phoenix: 869 anomalies\n",
      "  seattle: 1105 anomalies\n",
      "\n",
      "Sample weather anomalies:\n",
      "    City            DateTime anomaly_variable  lower_bound  upper_bound\n",
      "131   la 2018-07-06 17:00:00  Temperature (F)       36.705       92.905\n",
      "132   la 2018-07-06 18:00:00  Temperature (F)       36.705       92.905\n",
      "133   la 2018-07-06 19:00:00  Temperature (F)       36.705       92.905\n",
      "134   la 2018-07-06 20:00:00  Temperature (F)       36.705       92.905\n",
      "135   la 2018-07-06 21:00:00  Temperature (F)       36.705       92.905\n",
      "\n",
      "Found 0 physically impossible values\n",
      "\n",
      "==== GENERATING VISUALIZATIONS ====\n",
      "Saved demand anomaly plot for la to demand_anomalies_la.png\n",
      "Saved demand anomaly plot for san_diego to demand_anomalies_san_diego.png\n",
      "Saved demand anomaly plot for san_jose to demand_anomalies_san_jose.png\n",
      "Saved boxplot for Temperature (F) to weather_boxplot_Temperature_F.png\n",
      "Saved boxplot for Humidity to weather_boxplot_Humidity.png\n",
      "Saved boxplot for Wind Speed (mph) to weather_boxplot_Wind_Speed_mph.png\n",
      "Saved boxplot for Pressure to weather_boxplot_Pressure.png\n",
      "\n",
      "Analysis complete! Check the output files for visualizations.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('merged_weather_demand_final.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"First few rows:\")\n",
    "print(df.head(2))\n",
    "\n",
    "\n",
    "# Convert to datetime using 'Data Date' and 'Hour Number'\n",
    "print(\"\\nConverting date and time columns...\")\n",
    "df['DateTime'] = pd.to_datetime(df['Data Date'] + ' ' + df['Hour Number'].astype(str) + ':00', errors='coerce')\n",
    "\n",
    "# Check if DateTime conversion was successful\n",
    "print(f\"DateTime conversion successful: {not df['DateTime'].isna().all()}\")\n",
    "print(f\"Sample DateTime values: {df['DateTime'].head(3).tolist()}\")\n",
    "\n",
    "# 1. CHECK TIME INTERVAL CONSISTENCY\n",
    "print(\"\\n==== CHECKING TIME INTERVAL CONSISTENCY ====\")\n",
    "\n",
    "def check_time_intervals(city_data):\n",
    "    # Sort by datetime\n",
    "    city_data = city_data.sort_values('DateTime')\n",
    "    # Calculate time differences between consecutive rows\n",
    "    time_diffs = city_data['DateTime'].diff().dropna()\n",
    "    \n",
    "    if len(time_diffs) == 0:\n",
    "        return {\n",
    "            'consistent': None,\n",
    "            'modal_interval': None,\n",
    "            'irregular_count': 0,\n",
    "            'irregular_intervals': None\n",
    "        }\n",
    "    \n",
    "    # Most common time difference (should be 1 hour)\n",
    "    modal_diff = time_diffs.mode()[0]\n",
    "    # Check if all differences are the same\n",
    "    consistent = (time_diffs == modal_diff).all()\n",
    "    # Find any irregular intervals\n",
    "    irregular = time_diffs[time_diffs != modal_diff]\n",
    "    return {\n",
    "        'consistent': consistent,\n",
    "        'modal_interval': modal_diff,\n",
    "        'irregular_count': len(irregular),\n",
    "        'irregular_intervals': irregular if len(irregular) > 0 else None\n",
    "    }\n",
    "\n",
    "# Apply check to each city\n",
    "time_consistency = {}\n",
    "for city in df['City'].unique():\n",
    "    city_data = df[df['City'] == city]\n",
    "    time_consistency[city] = check_time_intervals(city_data)\n",
    "    \n",
    "print(\"Time interval consistency by city:\")\n",
    "for city, results in time_consistency.items():\n",
    "    print(f\"\\n{city}:\")\n",
    "    print(f\"  Consistent intervals: {results['consistent']}\")\n",
    "    print(f\"  Expected interval: {results['modal_interval']}\")\n",
    "    if results['irregular_count'] > 0:\n",
    "        print(f\"  Found {results['irregular_count']} irregular intervals\")\n",
    "        # Show sample of irregular intervals\n",
    "        sample_irregulars = results['irregular_intervals'].head(5)\n",
    "        print(f\"  Sample irregular intervals: {sample_irregulars.tolist()}\")\n",
    "\n",
    "# 2. CHECK FOR DUPLICATES\n",
    "print(\"\\n==== CHECKING FOR DUPLICATES ====\")\n",
    "\n",
    "duplicates = df.duplicated(subset=['City', 'DateTime'], keep=False)\n",
    "duplicate_records = df[duplicates].sort_values(['City', 'DateTime'])\n",
    "\n",
    "if len(duplicate_records) > 0:\n",
    "    print(f\"Found {len(duplicate_records)} duplicate records:\")\n",
    "    print(duplicate_records.head(10))  # Show first 10 duplicates\n",
    "    \n",
    "    # Count duplicates by city\n",
    "    dup_by_city = duplicate_records.groupby('City').size()\n",
    "    print(\"\\nDuplicates by city:\")\n",
    "    print(dup_by_city)\n",
    "else:\n",
    "    print(\"No duplicate records found.\")\n",
    "\n",
    "# 3. DETECT ANOMALIES\n",
    "print(\"\\n==== DETECTING ANOMALIES ====\")\n",
    "\n",
    "# Approach 1: Statistical method (Z-score) for demand anomalies by city\n",
    "def detect_demand_anomalies(df, threshold=3.0):\n",
    "    anomalies = pd.DataFrame()\n",
    "    anomaly_counts = {}\n",
    "    \n",
    "    for city in df['City'].unique():\n",
    "        city_data = df[df['City'] == city].copy()\n",
    "        \n",
    "        # Calculate z-scores for demand\n",
    "        city_data['demand_zscore'] = np.abs((city_data['Demand (MW)'] - \n",
    "                                          city_data['Demand (MW)'].mean()) / \n",
    "                                         city_data['Demand (MW)'].std())\n",
    "        \n",
    "        # Flag anomalies\n",
    "        city_anomalies = city_data[city_data['demand_zscore'] > threshold]\n",
    "        anomalies = pd.concat([anomalies, city_anomalies])\n",
    "        anomaly_counts[city] = len(city_anomalies)\n",
    "    \n",
    "    return anomalies, anomaly_counts\n",
    "\n",
    "# Approach 2: IQR method for weather variables\n",
    "def detect_weather_anomalies(df):\n",
    "    anomalies = pd.DataFrame()\n",
    "    anomaly_counts = {}\n",
    "    weather_vars = ['Temperature (F)', 'Humidity', 'Wind Speed (mph)', 'Pressure']\n",
    "    \n",
    "    for city in df['City'].unique():\n",
    "        city_data = df[df['City'] == city].copy()\n",
    "        city_anomalies = pd.DataFrame()\n",
    "        city_count = 0\n",
    "        \n",
    "        for var in weather_vars:\n",
    "            if var in city_data.columns:\n",
    "                Q1 = city_data[var].quantile(0.25)\n",
    "                Q3 = city_data[var].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                # Define bounds\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Flag outliers\n",
    "                outliers = city_data[(city_data[var] < lower_bound) | \n",
    "                                   (city_data[var] > upper_bound)].copy()\n",
    "                \n",
    "                if len(outliers) > 0:\n",
    "                    outliers['anomaly_variable'] = var\n",
    "                    outliers['lower_bound'] = lower_bound\n",
    "                    outliers['upper_bound'] = upper_bound\n",
    "                    city_anomalies = pd.concat([city_anomalies, outliers])\n",
    "                    city_count += len(outliers)\n",
    "        \n",
    "        if city_count > 0:\n",
    "            anomalies = pd.concat([anomalies, city_anomalies])\n",
    "            anomaly_counts[city] = city_count\n",
    "    \n",
    "    return anomalies, anomaly_counts\n",
    "\n",
    "# Approach 3: Check for physically impossible values\n",
    "def check_physical_constraints(df):\n",
    "    impossible_values = pd.DataFrame()\n",
    "    constraint_violations = {}\n",
    "    \n",
    "    # Define logical constraints\n",
    "    constraints = {\n",
    "        'Temperature (F)': (lambda x: (x < -50) | (x > 130), \"Temperature outside -50°F to 130°F\"),\n",
    "        'Humidity': (lambda x: (x < 0) | (x > 1), \"Humidity outside 0-1 range\"),\n",
    "        'Wind Speed (mph)': (lambda x: (x < 0) | (x > 200), \"Wind speed outside 0-200 mph\"),\n",
    "        'Pressure': (lambda x: (x < 900) | (x > 1100), \"Pressure outside 900-1100 range\"),\n",
    "        'Demand (MW)': (lambda x: x < 0, \"Negative demand\")\n",
    "    }\n",
    "    \n",
    "    for var, (constraint, msg) in constraints.items():\n",
    "        if var in df.columns:\n",
    "            invalid = df[constraint(df[var])].copy()\n",
    "            if len(invalid) > 0:\n",
    "                invalid['constraint_violation'] = msg\n",
    "                impossible_values = pd.concat([impossible_values, invalid])\n",
    "                constraint_violations[var] = len(invalid)\n",
    "    \n",
    "    return impossible_values, constraint_violations\n",
    "\n",
    "# Run the anomaly detection\n",
    "print(\"Detecting demand anomalies (using Z-score method)...\")\n",
    "demand_anomalies, demand_anomaly_counts = detect_demand_anomalies(df)\n",
    "\n",
    "print(\"Detecting weather anomalies (using IQR method)...\")\n",
    "weather_anomalies, weather_anomaly_counts = detect_weather_anomalies(df)\n",
    "\n",
    "print(\"Checking for physically impossible values...\")\n",
    "impossible_values, constraint_violations = check_physical_constraints(df)\n",
    "\n",
    "print(f\"\\nFound {len(demand_anomalies)} demand anomalies\")\n",
    "if len(demand_anomalies) > 0:\n",
    "    print(\"Demand anomalies by city:\")\n",
    "    for city, count in demand_anomaly_counts.items():\n",
    "        print(f\"  {city}: {count} anomalies\")\n",
    "    # Show sample of demand anomalies\n",
    "    print(\"\\nSample demand anomalies:\")\n",
    "    sample_cols = ['City', 'DateTime', 'Demand (MW)', 'demand_zscore']\n",
    "    print(demand_anomalies[sample_cols].head(5))\n",
    "\n",
    "print(f\"\\nFound {len(weather_anomalies)} weather anomalies\")\n",
    "if len(weather_anomalies) > 0:\n",
    "    print(\"Weather anomalies by city:\")\n",
    "    for city, count in weather_anomaly_counts.items():\n",
    "        print(f\"  {city}: {count} anomalies\")\n",
    "    # Show sample of weather anomalies\n",
    "    print(\"\\nSample weather anomalies:\")\n",
    "    sample_cols = ['City', 'DateTime', 'anomaly_variable', 'lower_bound', 'upper_bound']\n",
    "    print(weather_anomalies[sample_cols].head(5))\n",
    "\n",
    "print(f\"\\nFound {len(impossible_values)} physically impossible values\")\n",
    "if len(impossible_values) > 0:\n",
    "    print(\"Constraint violations by variable:\")\n",
    "    for var, count in constraint_violations.items():\n",
    "        print(f\"  {var}: {count} violations\")\n",
    "    # Show sample of impossible values\n",
    "    print(\"\\nSample impossible values:\")\n",
    "    sample_cols = ['City', 'DateTime', 'constraint_violation']\n",
    "    print(impossible_values[sample_cols].head(5))\n",
    "\n",
    "# 4. VISUALIZATION (save to files)\n",
    "print(\"\\n==== GENERATING VISUALIZATIONS ====\")\n",
    "\n",
    "# Plot demand time series with anomalies highlighted for a sample city\n",
    "def plot_demand_anomalies(df, city, anomalies, save_path):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    city_data = df[df['City'] == city]\n",
    "    city_anomalies = anomalies[anomalies['City'] == city]\n",
    "    \n",
    "    plt.plot(city_data['DateTime'], city_data['Demand (MW)'], 'b-', alpha=0.7)\n",
    "    if len(city_anomalies) > 0:\n",
    "        plt.scatter(city_anomalies['DateTime'], city_anomalies['Demand (MW)'], \n",
    "                    color='red', s=50, label='Anomalies')\n",
    "    \n",
    "    plt.title(f'Electricity Demand for {city} with Anomalies Highlighted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand (MW)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved demand anomaly plot for {city} to {save_path}\")\n",
    "\n",
    "# Boxplots for weather variables by city\n",
    "def plot_weather_boxplots(df, save_path):\n",
    "    weather_vars = ['Temperature (F)', 'Humidity', 'Wind Speed (mph)', 'Pressure']\n",
    "    for var in weather_vars:\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='City', y=var, data=df)\n",
    "        plt.title(f'Distribution of {var} by City')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        var_name = var.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        file_path = f\"{save_path}_{var_name}.png\"\n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved boxplot for {var} to {file_path}\")\n",
    "\n",
    "# Create visualizations for a few sample cities\n",
    "if len(df['City'].unique()) > 0:\n",
    "    for i, city in enumerate(df['City'].unique()[:3]):  # First 3 cities\n",
    "        plot_demand_anomalies(df, city, demand_anomalies, f\"demand_anomalies_{city}.png\")\n",
    "    \n",
    "    plot_weather_boxplots(df, \"weather_boxplot\")\n",
    "\n",
    "print(\"\\nAnalysis complete! Check the output files for visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1b2bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "Dataset shape: (165192, 22)\n",
      "First few rows:\n",
      "            Local Time Balancing Authority   Data Date  Hour Number  \\\n",
      "0  2018-07-01 06:00:00                CISO  2018-07-01          6.0   \n",
      "1  2018-07-01 07:00:00                CISO  2018-07-01          7.0   \n",
      "\n",
      "  Sub-Region  Demand (MW) City  Temperature (F)  Humidity  Wind Speed (mph)  \\\n",
      "0        SCE       9472.0   la            65.45      0.79              4.23   \n",
      "1        SCE       9353.0   la            65.45      0.79              4.23   \n",
      "\n",
      "   ... UV Index  Visibility  Cloud Cover  Hour  Day  Month  Weekday  Weekend  \\\n",
      "0  ...      0.0       9.798         0.25     6    1      7        6        1   \n",
      "1  ...      0.0       9.798         0.25     7    1      7        6        1   \n",
      "\n",
      "   Season             DateTime  \n",
      "0       3  2018-07-01 06:00:00  \n",
      "1       3  2018-07-01 07:00:00  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "\n",
      "Converting date and time columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leads trading\\AppData\\Local\\Temp\\ipykernel_3356\\3761659935.py:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['DateTime'] = pd.to_datetime(df['Data Date'] + ' ' + df['Hour Number'].astype(str) + ':00', errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime conversion successful: True\n",
      "Sample DateTime values: [Timestamp('2018-07-01 06:00:00'), Timestamp('2018-07-01 07:00:00'), Timestamp('2018-07-01 08:00:00')]\n",
      "\n",
      "==== CHECKING TIME INTERVAL CONSISTENCY ====\n",
      "Time interval consistency by city:\n",
      "\n",
      "la:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1376 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "san_diego:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1376 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "san_jose:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1376 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "dallas:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1374 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "houston:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1374 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "san_antonio:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1374 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "nyc:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1374 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "philadelphia:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1374 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "phoenix:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1376 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "seattle:\n",
      "  Consistent intervals: False\n",
      "  Expected interval: 0 days 01:00:00\n",
      "  Found 1376 irregular intervals\n",
      "  Sample irregular intervals: [Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00'), Timedelta('0 days 00:00:00'), Timedelta('0 days 02:00:00')]\n",
      "\n",
      "==== CHECKING FOR DUPLICATES ====\n",
      "Found 13750 duplicate records:\n",
      "                Local Time Balancing Authority   Data Date  Hour Number  \\\n",
      "49610  2018-07-03 01:00:00                ERCO  2018-07-03          1.0   \n",
      "49611  2018-07-03 01:00:00                ERCO  2018-07-03          1.0   \n",
      "49634  2018-07-04 01:00:00                ERCO  2018-07-04          1.0   \n",
      "49635  2018-07-04 01:00:00                ERCO  2018-07-04          1.0   \n",
      "49658  2018-07-05 01:00:00                ERCO  2018-07-05          1.0   \n",
      "49659  2018-07-05 01:00:00                ERCO  2018-07-05          1.0   \n",
      "49682  2018-07-06 01:00:00                ERCO  2018-07-06          1.0   \n",
      "49683  2018-07-06 01:00:00                ERCO  2018-07-06          1.0   \n",
      "49706  2018-07-07 01:00:00                ERCO  2018-07-07          1.0   \n",
      "49707  2018-07-07 01:00:00                ERCO  2018-07-07          1.0   \n",
      "\n",
      "      Sub-Region  Demand (MW)    City  Temperature (F)  Humidity  \\\n",
      "49610       NRTH         92.0  dallas          100.395     0.260   \n",
      "49611       NRTH         92.0  dallas           99.010     0.280   \n",
      "49634       NRTH         92.0  dallas          101.520     0.220   \n",
      "49635       NRTH         92.0  dallas          100.650     0.230   \n",
      "49658       NRTH         92.0  dallas           89.760     0.500   \n",
      "49659       NRTH         92.0  dallas           88.680     0.520   \n",
      "49682       NRTH         92.0  dallas           95.655     0.390   \n",
      "49683       NRTH         92.0  dallas           93.530     0.420   \n",
      "49706       NRTH         92.0  dallas           83.220     0.665   \n",
      "49707       NRTH         92.0  dallas           84.600     0.650   \n",
      "\n",
      "       Wind Speed (mph)  ... UV Index  Visibility  Cloud Cover  Hour  Day  \\\n",
      "49610             9.450  ...      1.0       9.997        0.020     0    3   \n",
      "49611             9.450  ...      0.0       9.997        0.000     1    3   \n",
      "49634             6.815  ...      1.0       9.997        0.095     0    4   \n",
      "49635             8.740  ...      0.0       9.997        0.150     1    4   \n",
      "49658            14.495  ...      1.0       9.997        0.685     0    5   \n",
      "49659            14.560  ...      0.0       9.997        0.620     1    5   \n",
      "49682             7.850  ...      1.0       9.997        0.535     0    6   \n",
      "49683             8.940  ...      0.0       9.997        0.440     1    6   \n",
      "49706             4.040  ...      1.0       9.997        0.290     0    7   \n",
      "49707             4.220  ...      0.0       9.997        0.180     1    7   \n",
      "\n",
      "       Month  Weekday  Weekend  Season            DateTime  \n",
      "49610      7        1        0       3 2018-07-03 01:00:00  \n",
      "49611      7        1        0       3 2018-07-03 01:00:00  \n",
      "49634      7        2        0       3 2018-07-04 01:00:00  \n",
      "49635      7        2        0       3 2018-07-04 01:00:00  \n",
      "49658      7        3        0       3 2018-07-05 01:00:00  \n",
      "49659      7        3        0       3 2018-07-05 01:00:00  \n",
      "49682      7        4        0       3 2018-07-06 01:00:00  \n",
      "49683      7        4        0       3 2018-07-06 01:00:00  \n",
      "49706      7        5        1       3 2018-07-07 01:00:00  \n",
      "49707      7        5        1       3 2018-07-07 01:00:00  \n",
      "\n",
      "[10 rows x 22 columns]\n",
      "\n",
      "Duplicates by city:\n",
      "City\n",
      "dallas          1374\n",
      "houston         1374\n",
      "la              1376\n",
      "nyc             1374\n",
      "philadelphia    1374\n",
      "phoenix         1376\n",
      "san_antonio     1374\n",
      "san_diego       1376\n",
      "san_jose        1376\n",
      "seattle         1376\n",
      "dtype: int64\n",
      "\n",
      "==== DETECTING ANOMALIES ====\n",
      "Detecting demand anomalies (using Z-score method)...\n",
      "Detecting weather anomalies (using IQR method)...\n",
      "Checking for physically impossible values...\n",
      "\n",
      "Found 823 demand anomalies\n",
      "Demand anomalies by city:\n",
      "  la: 297 anomalies\n",
      "  san_diego: 92 anomalies\n",
      "  san_jose: 138 anomalies\n",
      "  dallas: 0 anomalies\n",
      "  houston: 0 anomalies\n",
      "  san_antonio: 0 anomalies\n",
      "  nyc: 81 anomalies\n",
      "  philadelphia: 0 anomalies\n",
      "  phoenix: 215 anomalies\n",
      "  seattle: 0 anomalies\n",
      "\n",
      "Sample demand anomalies:\n",
      "    City            DateTime  Demand (MW)  demand_zscore\n",
      "107   la 2018-07-05 17:00:00      19120.0       3.221816\n",
      "108   la 2018-07-05 18:00:00      19402.0       3.339455\n",
      "109   la 2018-07-05 19:00:00      19144.0       3.231828\n",
      "126   la 2018-07-06 12:00:00      18682.0       3.039101\n",
      "137   la 2018-07-06 23:00:00      19568.0       3.408703\n",
      "\n",
      "Found 5169 weather anomalies\n",
      "Weather anomalies by city:\n",
      "  la: 341 anomalies\n",
      "  san_diego: 725 anomalies\n",
      "  san_jose: 451 anomalies\n",
      "  dallas: 429 anomalies\n",
      "  houston: 509 anomalies\n",
      "  san_antonio: 423 anomalies\n",
      "  nyc: 345 anomalies\n",
      "  philadelphia: 471 anomalies\n",
      "  phoenix: 649 anomalies\n",
      "  seattle: 826 anomalies\n",
      "\n",
      "Sample weather anomalies:\n",
      "    City            DateTime anomaly_variable  lower_bound  upper_bound\n",
      "131   la 2018-07-06 17:00:00  Temperature (F)      36.7425      92.8425\n",
      "132   la 2018-07-06 18:00:00  Temperature (F)      36.7425      92.8425\n",
      "143   la 2018-07-07 05:00:00  Temperature (F)      36.7425      92.8425\n",
      "144   la 2018-07-07 06:00:00  Temperature (F)      36.7425      92.8425\n",
      "157   la 2018-07-07 19:00:00  Temperature (F)      36.7425      92.8425\n",
      "\n",
      "Found 0 physically impossible values\n",
      "\n",
      "==== GENERATING VISUALIZATIONS ====\n",
      "Saved demand anomaly plot for la to demand_anomalies_la.png\n",
      "Saved demand anomaly plot for san_diego to demand_anomalies_san_diego.png\n",
      "Saved demand anomaly plot for san_jose to demand_anomalies_san_jose.png\n",
      "Saved boxplot for Temperature (F) to weather_boxplot_Temperature_F.png\n",
      "Saved boxplot for Humidity to weather_boxplot_Humidity.png\n",
      "Saved boxplot for Wind Speed (mph) to weather_boxplot_Wind_Speed_mph.png\n",
      "Saved boxplot for Pressure to weather_boxplot_Pressure.png\n",
      "\n",
      "Analysis complete! Check the output files for visualizations.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('merged_weather_demand_final.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"First few rows:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# From the sample data you provided, these should be the column names\n",
    "# Convert to datetime using 'Data Date' and 'Hour Number'\n",
    "print(\"\\nConverting date and time columns...\")\n",
    "df['DateTime'] = pd.to_datetime(df['Data Date'] + ' ' + df['Hour Number'].astype(str) + ':00', errors='coerce')\n",
    "\n",
    "# Check if DateTime conversion was successful\n",
    "print(f\"DateTime conversion successful: {not df['DateTime'].isna().all()}\")\n",
    "print(f\"Sample DateTime values: {df['DateTime'].head(3).tolist()}\")\n",
    "\n",
    "# 1. CHECK TIME INTERVAL CONSISTENCY\n",
    "print(\"\\n==== CHECKING TIME INTERVAL CONSISTENCY ====\")\n",
    "\n",
    "def check_time_intervals(city_data):\n",
    "    # Sort by datetime\n",
    "    city_data = city_data.sort_values('DateTime')\n",
    "    # Calculate time differences between consecutive rows\n",
    "    time_diffs = city_data['DateTime'].diff().dropna()\n",
    "    \n",
    "    if len(time_diffs) == 0:\n",
    "        return {\n",
    "            'consistent': None,\n",
    "            'modal_interval': None,\n",
    "            'irregular_count': 0,\n",
    "            'irregular_intervals': None\n",
    "        }\n",
    "    \n",
    "    # Most common time difference (should be 1 hour)\n",
    "    modal_diff = time_diffs.mode()[0]\n",
    "    # Check if all differences are the same\n",
    "    consistent = (time_diffs == modal_diff).all()\n",
    "    # Find any irregular intervals\n",
    "    irregular = time_diffs[time_diffs != modal_diff]\n",
    "    return {\n",
    "        'consistent': consistent,\n",
    "        'modal_interval': modal_diff,\n",
    "        'irregular_count': len(irregular),\n",
    "        'irregular_intervals': irregular if len(irregular) > 0 else None\n",
    "    }\n",
    "\n",
    "# Apply check to each city\n",
    "time_consistency = {}\n",
    "for city in df['City'].unique():\n",
    "    city_data = df[df['City'] == city]\n",
    "    time_consistency[city] = check_time_intervals(city_data)\n",
    "    \n",
    "print(\"Time interval consistency by city:\")\n",
    "for city, results in time_consistency.items():\n",
    "    print(f\"\\n{city}:\")\n",
    "    print(f\"  Consistent intervals: {results['consistent']}\")\n",
    "    print(f\"  Expected interval: {results['modal_interval']}\")\n",
    "    if results['irregular_count'] > 0:\n",
    "        print(f\"  Found {results['irregular_count']} irregular intervals\")\n",
    "        # Show sample of irregular intervals\n",
    "        sample_irregulars = results['irregular_intervals'].head(5)\n",
    "        print(f\"  Sample irregular intervals: {sample_irregulars.tolist()}\")\n",
    "\n",
    "# 2. CHECK FOR DUPLICATES\n",
    "print(\"\\n==== CHECKING FOR DUPLICATES ====\")\n",
    "\n",
    "duplicates = df.duplicated(subset=['City', 'DateTime'], keep=False)\n",
    "duplicate_records = df[duplicates].sort_values(['City', 'DateTime'])\n",
    "\n",
    "if len(duplicate_records) > 0:\n",
    "    print(f\"Found {len(duplicate_records)} duplicate records:\")\n",
    "    print(duplicate_records.head(10))  # Show first 10 duplicates\n",
    "    \n",
    "    # Count duplicates by city\n",
    "    dup_by_city = duplicate_records.groupby('City').size()\n",
    "    print(\"\\nDuplicates by city:\")\n",
    "    print(dup_by_city)\n",
    "else:\n",
    "    print(\"No duplicate records found.\")\n",
    "\n",
    "# 3. DETECT ANOMALIES\n",
    "print(\"\\n==== DETECTING ANOMALIES ====\")\n",
    "\n",
    "# Approach 1: Statistical method (Z-score) for demand anomalies by city\n",
    "def detect_demand_anomalies(df, threshold=3.0):\n",
    "    anomalies = pd.DataFrame()\n",
    "    anomaly_counts = {}\n",
    "    \n",
    "    for city in df['City'].unique():\n",
    "        city_data = df[df['City'] == city].copy()\n",
    "        \n",
    "        # Calculate z-scores for demand\n",
    "        city_data['demand_zscore'] = np.abs((city_data['Demand (MW)'] - \n",
    "                                          city_data['Demand (MW)'].mean()) / \n",
    "                                         city_data['Demand (MW)'].std())\n",
    "        \n",
    "        # Flag anomalies\n",
    "        city_anomalies = city_data[city_data['demand_zscore'] > threshold]\n",
    "        anomalies = pd.concat([anomalies, city_anomalies])\n",
    "        anomaly_counts[city] = len(city_anomalies)\n",
    "    \n",
    "    return anomalies, anomaly_counts\n",
    "\n",
    "# Approach 2: IQR method for weather variables\n",
    "def detect_weather_anomalies(df):\n",
    "    anomalies = pd.DataFrame()\n",
    "    anomaly_counts = {}\n",
    "    weather_vars = ['Temperature (F)', 'Humidity', 'Wind Speed (mph)', 'Pressure']\n",
    "    \n",
    "    for city in df['City'].unique():\n",
    "        city_data = df[df['City'] == city].copy()\n",
    "        city_anomalies = pd.DataFrame()\n",
    "        city_count = 0\n",
    "        \n",
    "        for var in weather_vars:\n",
    "            if var in city_data.columns:\n",
    "                Q1 = city_data[var].quantile(0.25)\n",
    "                Q3 = city_data[var].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                # Define bounds\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                # Flag outliers\n",
    "                outliers = city_data[(city_data[var] < lower_bound) | \n",
    "                                   (city_data[var] > upper_bound)].copy()\n",
    "                \n",
    "                if len(outliers) > 0:\n",
    "                    outliers['anomaly_variable'] = var\n",
    "                    outliers['lower_bound'] = lower_bound\n",
    "                    outliers['upper_bound'] = upper_bound\n",
    "                    city_anomalies = pd.concat([city_anomalies, outliers])\n",
    "                    city_count += len(outliers)\n",
    "        \n",
    "        if city_count > 0:\n",
    "            anomalies = pd.concat([anomalies, city_anomalies])\n",
    "            anomaly_counts[city] = city_count\n",
    "    \n",
    "    return anomalies, anomaly_counts\n",
    "\n",
    "# Approach 3: Check for physically impossible values\n",
    "def check_physical_constraints(df):\n",
    "    impossible_values = pd.DataFrame()\n",
    "    constraint_violations = {}\n",
    "    \n",
    "    # Define logical constraints\n",
    "    constraints = {\n",
    "        'Temperature (F)': (lambda x: (x < -50) | (x > 130), \"Temperature outside -50°F to 130°F\"),\n",
    "        'Humidity': (lambda x: (x < 0) | (x > 1), \"Humidity outside 0-1 range\"),\n",
    "        'Wind Speed (mph)': (lambda x: (x < 0) | (x > 200), \"Wind speed outside 0-200 mph\"),\n",
    "        'Pressure': (lambda x: (x < 900) | (x > 1100), \"Pressure outside 900-1100 range\"),\n",
    "        'Demand (MW)': (lambda x: x < 0, \"Negative demand\")\n",
    "    }\n",
    "    \n",
    "    for var, (constraint, msg) in constraints.items():\n",
    "        if var in df.columns:\n",
    "            invalid = df[constraint(df[var])].copy()\n",
    "            if len(invalid) > 0:\n",
    "                invalid['constraint_violation'] = msg\n",
    "                impossible_values = pd.concat([impossible_values, invalid])\n",
    "                constraint_violations[var] = len(invalid)\n",
    "    \n",
    "    return impossible_values, constraint_violations\n",
    "\n",
    "# Run the anomaly detection\n",
    "print(\"Detecting demand anomalies (using Z-score method)...\")\n",
    "demand_anomalies, demand_anomaly_counts = detect_demand_anomalies(df)\n",
    "\n",
    "print(\"Detecting weather anomalies (using IQR method)...\")\n",
    "weather_anomalies, weather_anomaly_counts = detect_weather_anomalies(df)\n",
    "\n",
    "print(\"Checking for physically impossible values...\")\n",
    "impossible_values, constraint_violations = check_physical_constraints(df)\n",
    "\n",
    "print(f\"\\nFound {len(demand_anomalies)} demand anomalies\")\n",
    "if len(demand_anomalies) > 0:\n",
    "    print(\"Demand anomalies by city:\")\n",
    "    for city, count in demand_anomaly_counts.items():\n",
    "        print(f\"  {city}: {count} anomalies\")\n",
    "    # Show sample of demand anomalies\n",
    "    print(\"\\nSample demand anomalies:\")\n",
    "    sample_cols = ['City', 'DateTime', 'Demand (MW)', 'demand_zscore']\n",
    "    print(demand_anomalies[sample_cols].head(5))\n",
    "\n",
    "print(f\"\\nFound {len(weather_anomalies)} weather anomalies\")\n",
    "if len(weather_anomalies) > 0:\n",
    "    print(\"Weather anomalies by city:\")\n",
    "    for city, count in weather_anomaly_counts.items():\n",
    "        print(f\"  {city}: {count} anomalies\")\n",
    "    # Show sample of weather anomalies\n",
    "    print(\"\\nSample weather anomalies:\")\n",
    "    sample_cols = ['City', 'DateTime', 'anomaly_variable', 'lower_bound', 'upper_bound']\n",
    "    print(weather_anomalies[sample_cols].head(5))\n",
    "\n",
    "print(f\"\\nFound {len(impossible_values)} physically impossible values\")\n",
    "if len(impossible_values) > 0:\n",
    "    print(\"Constraint violations by variable:\")\n",
    "    for var, count in constraint_violations.items():\n",
    "        print(f\"  {var}: {count} violations\")\n",
    "    # Show sample of impossible values\n",
    "    print(\"\\nSample impossible values:\")\n",
    "    sample_cols = ['City', 'DateTime', 'constraint_violation']\n",
    "    print(impossible_values[sample_cols].head(5))\n",
    "\n",
    "# 4. VISUALIZATION (save to files)\n",
    "print(\"\\n==== GENERATING VISUALIZATIONS ====\")\n",
    "\n",
    "# Plot demand time series with anomalies highlighted for a sample city\n",
    "def plot_demand_anomalies(df, city, anomalies, save_path):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    city_data = df[df['City'] == city]\n",
    "    city_anomalies = anomalies[anomalies['City'] == city]\n",
    "    \n",
    "    plt.plot(city_data['DateTime'], city_data['Demand (MW)'], 'b-', alpha=0.7)\n",
    "    if len(city_anomalies) > 0:\n",
    "        plt.scatter(city_anomalies['DateTime'], city_anomalies['Demand (MW)'], \n",
    "                    color='red', s=50, label='Anomalies')\n",
    "    \n",
    "    plt.title(f'Electricity Demand for {city} with Anomalies Highlighted')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand (MW)')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved demand anomaly plot for {city} to {save_path}\")\n",
    "\n",
    "# Boxplots for weather variables by city\n",
    "def plot_weather_boxplots(df, save_path):\n",
    "    weather_vars = ['Temperature (F)', 'Humidity', 'Wind Speed (mph)', 'Pressure']\n",
    "    for var in weather_vars:\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sns.boxplot(x='City', y=var, data=df)\n",
    "        plt.title(f'Distribution of {var} by City')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        var_name = var.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        file_path = f\"{save_path}_{var_name}.png\"\n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved boxplot for {var} to {file_path}\")\n",
    "\n",
    "# Create visualizations for a few sample cities\n",
    "if len(df['City'].unique()) > 0:\n",
    "    for i, city in enumerate(df['City'].unique()[:3]):  # First 3 cities\n",
    "        plot_demand_anomalies(df, city, demand_anomalies, f\"demand_anomalies_{city}.png\")\n",
    "    \n",
    "    plot_weather_boxplots(df, \"weather_boxplot\")\n",
    "\n",
    "print(\"\\nAnalysis complete! Check the output files for visualizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5749fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Fixing time intervals...\n",
      "\n",
      "Correcting demand anomalies...\n",
      "\n",
      "Correcting weather anomalies...\n",
      "\n",
      "Saving cleaned data...\n",
      "\n",
      "=== Cleaning Summary ===\n",
      "\n",
      "Demand Anomalies Corrected:\n",
      "- Corrected 1729 demand anomalies in seattle\n",
      "\n",
      "Weather Anomalies Corrected:\n",
      "\n",
      "Final dataset shape: (165192, 22)\n",
      "\n",
      "Data cleaning complete. Saved to merged_weather_demand_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import timedelta\n",
    "\n",
    "def load_and_preprocess(filepath):\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    df['Local Time'] = pd.to_datetime(df['Local Time'])\n",
    "    return df\n",
    "\n",
    "def fix_time_intervals(df):\n",
    "    \"\"\"Resample data to ensure consistent hourly intervals.\"\"\"\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    for city in df['City'].unique():\n",
    "        city_df = df[df['City'] == city].copy()\n",
    "        city_df = city_df.set_index('DateTime')\n",
    "        \n",
    "        # Create complete hourly index\n",
    "        full_range = pd.date_range(\n",
    "            start=city_df.index.min(),\n",
    "            end=city_df.index.max(),\n",
    "            freq='H'\n",
    "        )\n",
    "        \n",
    "        # Resample and interpolate\n",
    "        city_df = city_df.reindex(full_range)\n",
    "        \n",
    "        # Interpolate numerical columns\n",
    "        num_cols = ['Demand (MW)', 'Temperature (F)', 'Humidity', 'Wind Speed (mph)',\n",
    "                   'Pressure', 'UV Index', 'Visibility', 'Cloud Cover']\n",
    "        city_df[num_cols] = city_df[num_cols].interpolate(method='time')\n",
    "        \n",
    "        # Forward fill categorical columns\n",
    "        cat_cols = ['Balancing Authority', 'Sub-Region', 'Weather Summary']\n",
    "        city_df[cat_cols] = city_df[cat_cols].ffill()\n",
    "        \n",
    "        # Backfill remaining missing values\n",
    "        city_df = city_df.bfill()\n",
    "        \n",
    "        # Reset city info\n",
    "        city_df['City'] = city\n",
    "        city_df['Data Date'] = city_df.index.date\n",
    "        city_df['Hour'] = city_df.index.hour\n",
    "        city_df['Day'] = city_df.index.day\n",
    "        city_df['Month'] = city_df.index.month\n",
    "        city_df['Weekday'] = city_df.index.weekday\n",
    "        city_df['Weekend'] = city_df.index.weekday.isin([5,6]).astype(int)\n",
    "        city_df['Season'] = (city_df.index.month % 12 + 3) // 3\n",
    "        \n",
    "        cleaned_dfs.append(city_df.reset_index().rename(columns={'index':'DateTime'}))\n",
    "    \n",
    "    return pd.concat(cleaned_dfs, ignore_index=True)\n",
    "\n",
    "def correct_demand_anomalies(df):\n",
    "    \"\"\"Correct demand anomalies using city-specific thresholds.\"\"\"\n",
    "    df = df.copy()\n",
    "    demand_log = []\n",
    "    \n",
    "    for city in df['City'].unique():\n",
    "        city_mask = df['City'] == city\n",
    "        temp_df = df[city_mask].copy()\n",
    "        temp_df = temp_df.set_index('DateTime')\n",
    "        \n",
    "        # Calculate robust statistics\n",
    "        q1, q3 = np.percentile(temp_df['Demand (MW)'], [25, 75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 3 * iqr\n",
    "        upper_bound = q3 + 3 * iqr\n",
    "        \n",
    "        # Find anomalies\n",
    "        anomalies = (temp_df['Demand (MW)'] < lower_bound) | (temp_df['Demand (MW)'] > upper_bound)\n",
    "        anomaly_count = anomalies.sum()\n",
    "        \n",
    "        if anomaly_count > 0:\n",
    "            for dt in temp_df.index[anomalies]:\n",
    "                hour = dt.hour\n",
    "                date_range = (dt - timedelta(days=3), dt + timedelta(days=3))\n",
    "                \n",
    "                # Get same hour from ±3 days\n",
    "                window_mask = (\n",
    "                    (temp_df.index.hour == hour) & \n",
    "                    (temp_df.index >= date_range[0]) & \n",
    "                    (temp_df.index <= date_range[1])\n",
    "                )\n",
    "                window = temp_df[window_mask]\n",
    "                \n",
    "                if len(window) > 0:\n",
    "                    median_val = window['Demand (MW)'].median()\n",
    "                    temp_df.loc[dt, 'Demand (MW)'] = median_val\n",
    "            \n",
    "            # Update original DataFrame\n",
    "            df.loc[city_mask, 'Demand (MW)'] = temp_df['Demand (MW)'].values\n",
    "            demand_log.append(f\"Corrected {anomaly_count} demand anomalies in {city}\")\n",
    "    \n",
    "    return df, demand_log\n",
    "\n",
    "def correct_weather_anomalies(df):\n",
    "    \"\"\"Correct weather anomalies using physical constraints.\"\"\"\n",
    "    df = df.copy()\n",
    "    weather_log = []\n",
    "    \n",
    "    # City-specific temperature bounds (Fahrenheit)\n",
    "    temp_bounds = {\n",
    "        'la': (40, 110),\n",
    "        'san_diego': (40, 105),\n",
    "        'san_jose': (35, 105),\n",
    "        'dallas': (10, 110),\n",
    "        'houston': (20, 105),\n",
    "        'san_antonio': (25, 108),\n",
    "        'nyc': (10, 100),\n",
    "        'philadelphia': (10, 100),\n",
    "        'phoenix': (40, 120),\n",
    "        'seattle': (30, 95)\n",
    "    }\n",
    "    \n",
    "    for city in df['City'].unique():\n",
    "        city_mask = df['City'] == city\n",
    "        temp_df = df[city_mask].copy()\n",
    "        temp_df = temp_df.set_index('DateTime')\n",
    "        \n",
    "        # Fix temperature anomalies\n",
    "        lower, upper = temp_bounds[city]\n",
    "        temp_anomalies = (temp_df['Temperature (F)'] < lower) | (temp_df['Temperature (F)'] > upper)\n",
    "        temp_count = temp_anomalies.sum()\n",
    "        \n",
    "        if temp_count > 0:\n",
    "            temp_df.loc[temp_anomalies, 'Temperature (F)'] = np.nan\n",
    "            temp_df['Temperature (F)'] = temp_df['Temperature (F)'].interpolate(method='time')\n",
    "            weather_log.append(f\"Corrected {temp_count} temperature anomalies in {city}\")\n",
    "        \n",
    "        # Fix other weather variables\n",
    "        weather_vars = ['Humidity', 'Wind Speed (mph)', 'Pressure']\n",
    "        for var in weather_vars:\n",
    "            # Calculate IQR bounds\n",
    "            q1, q3 = np.percentile(temp_df[var].dropna(), [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 3 * iqr\n",
    "            upper_bound = q3 + 3 * iqr\n",
    "            \n",
    "            # Find anomalies\n",
    "            anomalies = (temp_df[var] < lower_bound) | (temp_df[var] > upper_bound)\n",
    "            anomaly_count = anomalies.sum()\n",
    "            \n",
    "            if anomaly_count > 0:\n",
    "                temp_df.loc[anomalies, var] = np.nan\n",
    "                temp_df[var] = temp_df[var].interpolate(method='time')\n",
    "                weather_log.append(f\"Corrected {anomaly_count} {var} anomalies in {city}\")\n",
    "        \n",
    "        # Update original DataFrame\n",
    "        for col in ['Temperature (F)'] + weather_vars:\n",
    "            df.loc[city_mask, col] = temp_df[col].values\n",
    "    \n",
    "    return df, weather_log\n",
    "\n",
    "def save_cleaned_data(df, filepath):\n",
    "    \"\"\"Save cleaned data to CSV.\"\"\"\n",
    "    # Reorder columns to match original structure\n",
    "    original_order = [\n",
    "        'Local Time', 'Balancing Authority', 'Data Date', 'Hour Number', 'Sub-Region',\n",
    "        'Demand (MW)', 'City', 'Temperature (F)', 'Humidity', 'Wind Speed (mph)',\n",
    "        'Weather Summary', 'Pressure', 'UV Index', 'Visibility', 'Cloud Cover',\n",
    "        'Hour', 'Day', 'Month', 'Weekday', 'Weekend', 'Season', 'DateTime'\n",
    "    ]\n",
    "    df = df[original_order]\n",
    "    df.to_csv(filepath, index=False)\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess(\"merged_weather_demand_final.csv\")\n",
    "    \n",
    "    # Fix time intervals\n",
    "    print(\"\\nFixing time intervals...\")\n",
    "    df = fix_time_intervals(df)\n",
    "    \n",
    "    # Correct demand anomalies\n",
    "    print(\"\\nCorrecting demand anomalies...\")\n",
    "    df, demand_log = correct_demand_anomalies(df)\n",
    "    \n",
    "    # Correct weather anomalies\n",
    "    print(\"\\nCorrecting weather anomalies...\")\n",
    "    df, weather_log = correct_weather_anomalies(df)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    print(\"\\nSaving cleaned data...\")\n",
    "    save_cleaned_data(df, \"merged_weather_demand_final.csv\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Cleaning Summary ===\")\n",
    "    print(\"\\nDemand Anomalies Corrected:\")\n",
    "    for log in demand_log:\n",
    "        print(f\"- {log}\")\n",
    "    \n",
    "    print(\"\\nWeather Anomalies Corrected:\")\n",
    "    for log in weather_log:\n",
    "        print(f\"- {log}\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "    print(\"\\nData cleaning complete. Saved to merged_weather_demand_final.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starting data verification...\n",
      "\n",
      "=== VERIFICATION SUMMARY ===\n",
      "\n",
      "TIMESTAMP_CHECKS RESULTS:\n",
      "- la_missing_hours: 0\n",
      "- san_diego_missing_hours: 0\n",
      "- san_jose_missing_hours: 0\n",
      "- dallas_missing_hours: 0\n",
      "- houston_missing_hours: 0\n",
      "- san_antonio_missing_hours: 0\n",
      "- nyc_missing_hours: 0\n",
      "- philadelphia_missing_hours: 0\n",
      "- phoenix_missing_hours: 0\n",
      "- seattle_missing_hours: 0\n",
      "- duplicate_records: 0\n",
      "- irregular_intervals: 0\n",
      "\n",
      "VALUE_RANGES RESULTS:\n",
      "- demand_stats: {'min': 58.0, 'max': 20017.0, 'mean': 4163.547514407477}\n",
      "- temperature_ranges: {'dallas': {'min': 22.24, 'max': 109.31}, 'houston': {'min': 30.15, 'max': 100.84}, 'la': {'min': 40.49, 'max': 98.92}, 'nyc': {'min': 10.46, 'max': 96.25}, 'philadelphia': {'min': 10.24, 'max': 97.93}, 'phoenix': {'min': 40.0, 'max': 114.03}, 'san_antonio': {'min': 25.51, 'max': 106.11}, 'san_diego': {'min': 40.12, 'max': 90.63}, 'san_jose': {'min': 35.06, 'max': 93.41}, 'seattle': {'min': 30.02, 'max': 90.32}}\n",
      "- invalid_humidity: 0\n",
      "\n",
      "MISSING_DATA RESULTS:\n",
      "- missing_Demand (MW): 0\n",
      "- missing_Temperature (F): 0\n",
      "- missing_Humidity: 0\n",
      "- missing_Wind Speed (mph): 0\n",
      "- missing_Pressure: 0\n",
      "- missing_UV Index: 0\n",
      "\n",
      "ANOMALY_CHECK RESULTS:\n",
      "- la_demand_anomalies: 297\n",
      "- san_diego_demand_anomalies: 92\n",
      "- san_jose_demand_anomalies: 138\n",
      "- dallas_demand_anomalies: 0\n",
      "- houston_demand_anomalies: 0\n",
      "- san_antonio_demand_anomalies: 0\n",
      "- nyc_demand_anomalies: 81\n",
      "- philadelphia_demand_anomalies: 0\n",
      "- phoenix_demand_anomalies: 215\n",
      "- seattle_demand_anomalies: 0\n",
      "- Temperature (F)_anomalies: 0\n",
      "- Humidity_anomalies: 0\n",
      "- Wind Speed (mph)_anomalies: 0\n",
      "\n",
      "CITY_CONSISTENCY RESULTS:\n",
      "- date_ranges: {'dallas': {'min': Timestamp('2018-07-02 04:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16508}, 'houston': {'min': Timestamp('2018-07-02 04:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16508}, 'la': {'min': Timestamp('2018-07-01 06:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16530}, 'nyc': {'min': Timestamp('2018-07-02 03:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16509}, 'philadelphia': {'min': Timestamp('2018-07-02 03:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16509}, 'phoenix': {'min': Timestamp('2018-07-01 06:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16530}, 'san_antonio': {'min': Timestamp('2018-07-02 04:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16508}, 'san_diego': {'min': Timestamp('2018-07-01 06:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16530}, 'san_jose': {'min': Timestamp('2018-07-01 06:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16530}, 'seattle': {'min': Timestamp('2018-07-01 06:00:00'), 'max': Timestamp('2020-05-19 23:00:00'), 'count': 16530}}\n",
      "- consistent_date_ranges: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def verify_cleaned_data(filepath):\n",
    "    \"\"\"Run comprehensive verification checks on cleaned data\"\"\"\n",
    "    print(\" Starting data verification...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    df['Local Time'] = pd.to_datetime(df['Local Time'])\n",
    "    \n",
    "    verification_results = {\n",
    "        'timestamp_checks': verify_timestamps(df),\n",
    "        'value_ranges': verify_value_ranges(df),\n",
    "        'missing_data': verify_missing_data(df),\n",
    "        'anomaly_check': verify_anomalies(df),\n",
    "        'city_consistency': verify_city_consistency(df)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== VERIFICATION SUMMARY ===\")\n",
    "    for check_name, results in verification_results.items():\n",
    "        print(f\"\\n{check_name.upper()} RESULTS:\")\n",
    "        if isinstance(results, dict):\n",
    "            for k, v in results.items():\n",
    "                print(f\"- {k}: {v}\")\n",
    "        else:\n",
    "            print(results)\n",
    "    \n",
    "    return verification_results\n",
    "\n",
    "def verify_timestamps(df):\n",
    "    \"\"\"Verify timestamp continuity and consistency\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check for missing hours\n",
    "    for city in df['City'].unique():\n",
    "        city_df = df[df['City'] == city].set_index('DateTime')\n",
    "        expected_hours = pd.date_range(\n",
    "            start=city_df.index.min(),\n",
    "            end=city_df.index.max(),\n",
    "            freq='H'\n",
    "        )\n",
    "        missing = expected_hours.difference(city_df.index)\n",
    "        results[f\"{city}_missing_hours\"] = len(missing)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated(subset=['City', 'DateTime'], keep=False).sum()\n",
    "    results['duplicate_records'] = duplicates\n",
    "    \n",
    "    # Check time intervals\n",
    "    interval_issues = 0\n",
    "    for city in df['City'].unique():\n",
    "        city_df = df[df['City'] == city].sort_values('DateTime')\n",
    "        time_diffs = city_df['DateTime'].diff().dropna()\n",
    "        irregular = time_diffs[time_diffs != timedelta(hours=1)]\n",
    "        interval_issues += len(irregular)\n",
    "    results['irregular_intervals'] = interval_issues\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_value_ranges(df):\n",
    "    \"\"\"Verify all values are within reasonable ranges\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Demand checks\n",
    "    demand_stats = df['Demand (MW)'].agg(['min', 'max', 'mean'])\n",
    "    results['demand_stats'] = demand_stats.to_dict()\n",
    "    \n",
    "    # Temperature checks\n",
    "    temp_stats = df.groupby('City')['Temperature (F)'].agg(['min', 'max'])\n",
    "    results['temperature_ranges'] = temp_stats.to_dict('index')\n",
    "    \n",
    "    # Humidity checks (0-100%)\n",
    "    bad_humidity = df[(df['Humidity'] < 0) | (df['Humidity'] > 1)].shape[0]\n",
    "    results['invalid_humidity'] = bad_humidity\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_missing_data(df):\n",
    "    \"\"\"Check for remaining missing values\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    numeric_cols = ['Demand (MW)', 'Temperature (F)', 'Humidity', \n",
    "                   'Wind Speed (mph)', 'Pressure', 'UV Index']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        missing = df[col].isna().sum()\n",
    "        results[f\"missing_{col}\"] = missing\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_anomalies(df, z_threshold=3):\n",
    "    \"\"\"Check for remaining statistical anomalies\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Demand anomalies\n",
    "    for city in df['City'].unique():\n",
    "        city_demand = df[df['City'] == city]['Demand (MW)']\n",
    "        z_scores = np.abs((city_demand - city_demand.mean()) / city_demand.std())\n",
    "        anomalies = z_scores > z_threshold\n",
    "        results[f\"{city}_demand_anomalies\"] = anomalies.sum()\n",
    "    \n",
    "    # Weather anomalies\n",
    "    weather_vars = ['Temperature (F)', 'Humidity', 'Wind Speed (mph)']\n",
    "    for var in weather_vars:\n",
    "        anomalies = 0\n",
    "        for city in df['City'].unique():\n",
    "            city_data = df[df['City'] == city][var]\n",
    "            q1, q3 = np.percentile(city_data, [25, 75])\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 3*iqr\n",
    "            upper = q3 + 3*iqr\n",
    "            anomalies += ((city_data < lower) | (city_data > upper)).sum()\n",
    "        results[f\"{var}_anomalies\"] = anomalies\n",
    "    \n",
    "    return results\n",
    "\n",
    "def verify_city_consistency(df):\n",
    "    \"\"\"Verify all cities have consistent data coverage\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    city_stats = df.groupby('City')['DateTime'].agg(['min', 'max', 'count'])\n",
    "    results['date_ranges'] = city_stats.to_dict('index')\n",
    "    \n",
    "    # Check if all cities have same date range\n",
    "    start_dates = city_stats['min'].unique()\n",
    "    end_dates = city_stats['max'].unique()\n",
    "    results['consistent_date_ranges'] = len(start_dates) == 1 and len(end_dates) == 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verification_results = verify_cleaned_data(\"merged_weather_demand_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d264b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Local Time' removed and file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_column_and_save(filepath, column_to_remove='Local Time'):\n",
    "    \"\"\"\n",
    "    Remove specified column from CSV and save back to same file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the CSV file\n",
    "        column_to_remove (str): Name of column to remove (default: 'Local Time')\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Check if column exists\n",
    "    if column_to_remove not in df.columns:\n",
    "        print(f\"Column '{column_to_remove}' not found in the file.\")\n",
    "        print(\"Available columns:\", list(df.columns))\n",
    "        return\n",
    "    \n",
    "    # Remove the column\n",
    "    df = df.drop(columns=[column_to_remove])\n",
    "    \n",
    "    # Save back to same CSV file\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Column '{column_to_remove}' removed and file saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual file path\n",
    "    csv_file = \"merged_weather_demand_final.csv\"\n",
    "    remove_column_and_save(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36eba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum and Maximum values for each column:\n",
      "\n",
      "Column                    Min             Max            \n",
      "-------------------------------------------------------\n",
      "Balancing Authority       'CISO'          'PNM'          \n",
      "Date                      689 unique      values         \n",
      "Hour Number               1               23             \n",
      "Sub-Region                'NRTH'          'ZONA'         \n",
      "Demand (MW)               58.0            20017.0        \n",
      "City                      'dallas'        'seattle'      \n",
      "Temperature (F)           10.24           114.03         \n",
      "Humidity                  0.0             1.0            \n",
      "Wind Speed (mph)          0.0             21.9           \n",
      "Weather Summary           39 unique       values         \n",
      "Pressure                  990.7           1042.1         \n",
      "UV Index                  0.0             12.0           \n",
      "Visibility                0.215           10.0           \n",
      "Cloud Cover               0.0             1.0            \n",
      "Hour                      0               23             \n",
      "Day                       1               31             \n",
      "Month                     1               12             \n",
      "Weekday                   0               6              \n",
      "Weekend                   0               1              \n",
      "Season                    1               4              \n",
      "DateTime                  16530 unique    values         \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def print_min_max_values(filepath):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(\"Minimum and Maximum values for each column:\\n\")\n",
    "    print(\"{:<25} {:<15} {:<15}\".format('Column', 'Min', 'Max'))\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            # For numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                min_val = df[column].min()\n",
    "                max_val = df[column].max()\n",
    "                print(\"{:<25} {:<15} {:<15}\".format(column, min_val, max_val))\n",
    "            \n",
    "            # For datetime columns\n",
    "            elif pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "                min_val = df[column].min()\n",
    "                max_val = df[column].max()\n",
    "                print(\"{:<25} {:<15} {:<15}\".format(column, min_val, max_val))\n",
    "            \n",
    "            # For string/categorical columns\n",
    "            else:\n",
    "                unique_count = df[column].nunique()\n",
    "                if unique_count <= 10:  # Show values if few unique\n",
    "                    min_val = df[column].min()\n",
    "                    max_val = df[column].max()\n",
    "                    print(\"{:<25} {:<15} {:<15}\".format(column, f\"'{min_val}'\", f\"'{max_val}'\"))\n",
    "                else:\n",
    "                    print(\"{:<25} {:<15} {:<15}\".format(column, f\"{unique_count} unique\", \"values\"))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"{:<25} {}\".format(column, f\"Error: {str(e)}\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual file path\n",
    "    csv_file = \"merged_weather_demand_final.csv\"\n",
    "    print_min_max_values(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ba31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers have been replaced with the mean. The updated file has been saved back to 'merged_weather_demand_final.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('merged_weather_demand_final.csv')\n",
    "\n",
    "# Function to replace outliers with column mean\n",
    "def replace_outliers_with_mean(df, column):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    \n",
    "    # Calculate the IQR (Interquartile Range)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the bounds for non-outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Replace outliers with the mean of the column\n",
    "    df[column] = df[column].apply(lambda x: df[column].mean() if x < lower_bound or x > upper_bound else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Replace outliers in 'Demand (MW)' and 'Temperature (F)' columns\n",
    "df = replace_outliers_with_mean(df, 'Demand (MW)')\n",
    "df = replace_outliers_with_mean(df, 'Temperature (F)')\n",
    "\n",
    "# Save the modified DataFrame back to the original CSV file\n",
    "df.to_csv('merged_weather_demand_final.csv', index=False)\n",
    "\n",
    "print(\"Outliers have been replaced with the mean. The updated file has been saved back to 'merged_weather_demand_final.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fbc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully computed weekly statistics with columns:\n",
      "['City_', 'Balancing Authority_', 'Sub-Region_', 'Week_Start_Date', 'Avg_Demand_MW', 'Peak_Demand_MW', 'Min_Demand_MW', 'Total_Weekly_Demand_MWh', 'Demand_StDev', 'Median_Demand_MW', 'Avg_Temp_F', 'Max_Temp_F', 'Min_Temp_F', 'Avg_Humidity', 'Avg_Wind_Speed_mph', 'Weekend_Hours_Count', 'Demand_Variability', 'Peak_to_Avg_Ratio']\n"
     ]
    }
   ],
   "source": [
    "#AGGREGATION\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('merged_weather_demand_final.csv')\n",
    "\n",
    "# Ensure proper datetime conversion\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "\n",
    "# Weekly aggregation\n",
    "weekly_stats = df.groupby(['City', 'Balancing Authority', 'Sub-Region', \n",
    "                          pd.Grouper(key='DateTime', freq='W-MON')]).agg({\n",
    "    'Demand (MW)': ['mean', 'max', 'min', 'sum', 'std', 'median'],\n",
    "    'Temperature (F)': ['mean', 'max', 'min'],\n",
    "    'Humidity': 'mean',\n",
    "    'Wind Speed (mph)': 'mean',\n",
    "    'Weekend': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# FLATTEN COLUMN NAMES PROPERLY\n",
    "weekly_stats.columns = ['_'.join(col).strip() for col in weekly_stats.columns.values]\n",
    "weekly_stats = weekly_stats.rename(columns={\n",
    "    'DateTime_': 'Week_Start_Date',\n",
    "    'Demand (MW)_mean': 'Avg_Demand_MW',\n",
    "    'Demand (MW)_max': 'Peak_Demand_MW',\n",
    "    'Demand (MW)_min': 'Min_Demand_MW',\n",
    "    'Demand (MW)_sum': 'Total_Weekly_Demand_MWh',\n",
    "    'Demand (MW)_std': 'Demand_StDev',\n",
    "    'Demand (MW)_median': 'Median_Demand_MW',\n",
    "    'Temperature (F)_mean': 'Avg_Temp_F',\n",
    "    'Temperature (F)_max': 'Max_Temp_F',\n",
    "    'Temperature (F)_min': 'Min_Temp_F',\n",
    "    'Humidity_mean': 'Avg_Humidity',\n",
    "    'Wind Speed (mph)_mean': 'Avg_Wind_Speed_mph',\n",
    "    'Weekend_sum': 'Weekend_Hours_Count'\n",
    "})\n",
    "\n",
    "# Now safely calculate derived metrics\n",
    "weekly_stats['Demand_Variability'] = weekly_stats['Demand_StDev'] / weekly_stats['Avg_Demand_MW']\n",
    "weekly_stats['Peak_to_Avg_Ratio'] = weekly_stats['Peak_Demand_MW'] / weekly_stats['Avg_Demand_MW']\n",
    "\n",
    "# Save the results\n",
    "#weekly_stats.to_csv('weekly_demand_stats_corrected.csv', index=False)\n",
    "\n",
    "print(\"Successfully computed weekly statistics with columns:\")\n",
    "print(weekly_stats.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
